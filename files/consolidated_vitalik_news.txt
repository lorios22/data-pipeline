Consolidated News
================================================================================

Title: 2019 Was The Year of DeFi (and Why 2020 Will be Too) | Consensys
Date: Date not available
Content:
Products   Ecosystem   Company   Blog   Article 2019 Was The Year of DeFi (and Why 2020 Will be Too) Date December 5, 2019 Author Mason Nystrom How Ethereum will create an open financial system with new financial assets and protocols.  Reading time 16 min. read Share So far in the story of blockchain, every year can be distilled to one overarching trend. In 2009, the narrative was Bitcoin. In 2015, Ethereum. 2017 brought the world initial coin offerings. The answer for 2019 is unmistakable: the decentralized finance movement was by far the most impactful trend within the crypto and blockchain ecosystem.  Google searches for “decentralised finance” surged 273% in 2019 compared to 2018, according to our seo consultant: Victoria Olsina. In order to understand why 2019 was the year of DeFi, let’s break down what makes the open financial movement so revolutionary, and examine the strides the phenomenon has made towards creating an open financial system.  Open finance—or decentralized finance—refers to the paradigm shift from today’s closed financial system towards an open financial economy based on open protocols that are interoperable, programmable, and composable.  As the crypto ecosystem looks to expand, the term Open Finance more accurately describes the intended destination, because Ethereum is creating a new on-chain economy that integrates with current financial systems. Open finance is not about creating a new system from scratch, it’s about democratizing the existing system and making it more equitable using open protocols and transparent data.  Interoperable The current financial system is comprised of walled gardens with limited transferability or two-way access. Where interoperability is possible, it’s controlled by middlemen and rent-seekers. Open finance is defined by platforms that can work together with a degree of transparency with functions that complement one another.  Programmable Bitcoin revolutionized money. Ethereum has enabled new types of financial instruments and assets that are more customizable than existing products and services. Digital assets and securities will usher in a new era of financial mechanisms and growth.    Composable For many, a favorite childhood toy growing up are Legos, with which what you can build is only limited by your imagination. Composability refers to the concept that something can be selected and assembled in multiple combinations (i.e., Legos). Ethereum has shown the value of composability, having already become a protocol for other protocols such as Maker, UMA, Augur, Compound, and many more.  Every financial system requires numerous traits to become an economic engine, but first, we must define those criteria. There are a few generally accepted characteristics that comprise any economic system.  A functioning financial system contains global, efficient markets that allow for:  The ability for borrowers and lenders to transfer capital A process of exchange and trading (i.e., derivatives) with the necessary liquidity Means for individuals/investors to move, save, formulate, and allocate capital The capacity to offset and manage risk And provides regulatory protections for investors and individuals So a new financial system that is open must meet these attributes while removing the typical intermediaries or centralized points of control. Let’s examine the above characteristics and how they compare to the Ethereum ecosystem.  For a financial system to provide suitable market liquidity for borrowers, it needs lenders – and to obtain lenders a system requires borrowers. Chicken, meet the egg. This problem is initially solved via speculation. Crypto, which is a speculative asset class, has managed to create a few interesting lending and borrowing protocols.  MakerDAO was the first protocol to create a means by which individuals could continue to speculate through collateralization. While MakerDAO vaults (formerly called CDPs) have enabled users to borrow dai using ether as collateral, new lending and borrowing protocols dominated much of the 2019 crypto narrative. Compound and Fulcrum both create pools of capital allowing users to lend or borrow cryptoassets including dai, USDC (Coinbase’s stablecoin), ether, and more.    Dharma was originally competing with Compound now leverages Compound’s protocol to provide its customers with the best interest rate possible. Dharma achieves this through its “smart wallet” which automatically deposits customers’ funds into the Compound protocol.  Most recently, MakerDAO’s highly anticipated upgrade from single collateral dai (Sai) to multi-collateral dai (MCD) was successfully completed. The Maker protocol upgrade also came with a new dai savings rate (DSR), which provides interest to individuals who keep their dai in the Maker protocol. In the short term, the DSR is a smart contract that can be integrated into any other exchange and may become the base interest rate for the crypto DeFi space.  Long term and in the most bullish of circumstances, the DSR has the potential to become a trust-free savings rate similar to how treasury bills act as the risk-free rate. The distinction between trust-free and risk-free is subtle yet distinct. While Maker’s protocol comes with elements of risk such as smart contract risk, it does not require trust from any single entity (the protocol is governed by MKR token holders). Eliminating the reliance on large organizations, including the government is a powerful tool for reducing systemic risk.  These various protocols showcase the power of composability: 1)Dharma provides an open platform by 2) leveraging Compound’s open protocol, 3) which predominantly lends and borrows the dai stablecoin created by MakerDAO’s protocol, 4) all on top of Ethereum. Boom.  Financial systems operate through open markets and require robust mechanisms for trading and transferring value. The 2019 trading landscape was primarily dominated by a few large players, predominantly Coinbase, Kraken, Gemini, Bitstamp, Bitfinex, and Binance.  This past year showcased crypto exchanges are primed to become the user interfaces for investing, trading, and other activities. Most major exchanges offered dozens of new trading pairs and developed new products or services such as mobile wallets, educational tools, and token products. Similar to Apple product stickiness, crypto customers will probably remain loyal to one or two crypto exchanges because of the lengthy onboarding process and people’s desire to custody assets in a single location.  Notably, a few exchanges decided to focus on capturing institutional clients, building out custody solutions or acquiring the necessary infrastructure. While centralized trading has been widely popular (and profitable), it’s not quite the new financial economy most crypto enthusiasts and entrepreneurs are hoping to achieve.  Uniswap emerged this year as the go-to decentralized exchange (DEX) for trading cryptocurrencies in a disintermediated fashion. Uniswap reported trading volume over $8 million in a twenty-four period in November of 2019. Uniswap currently comprises 33% of all DEX volume which exceeds IDEX and Kyber. Another decentralized exchange dYdX developed a platform combining trading, borrowing, and lending. The dYdX DEX aggregates spot prices and lending liquidity across multiple exchanges for its users. Notably, Binance also offers customers lending capabilities on its centralized exchange.  Initial Exchange Offerings (IEOs) were incredibly popular in the first half of 2019, where a centralized exchange served as the platform for token sales. Essentially, An exchange acts as the gatekeeper for these token sales and only verified customers are allowed to participate. The token sales were usually conducted using an exchange’s token such as Binance Coin (BNB) which offers reduced trading fees and stock like buy-backs in which they burn tokens to effectively increase the price for holders. A report by Token Insights halfway through 2019 revealed the IEOs initial hype throughout the first couple of months of 2019 before slowly fading in popularity. While IEOs, ICOs, and STOs (security token offerings) have had their fair share of scams and misuses, the ability to raise capital is a necessary tool for any financial system.  In the definition laid out in the beginning, the ability to formulate capital and move money were also present. Ethereum has mostly solved these two issues through first natively through the ability to transfer ether and secondly through security or token offerings as mentioned above.  Investing in cryptoassets has certainly evolved throughout 2019 and will continue to expand in the coming years. Speculation remains the most prominent use case for cryptocurrencies, however, many individuals perceive this trait as entirely negative. Tools and mechanisms for speculation are a vital component for any new asset class, especially an asset aiming to be the bedrock of an open financial system. One of the most important and subsequently hardest aspects to creating a new financial system is generating the necessary liquidity required for efficient investing.  Liquidity is the capacity for an investor to convert any asset into cash quickly. The term also refers to an individual’s ability to buy or sell a financial instrument without affecting the asset's price. This is obviously incredibly difficult when the value of the entire crypto-economy is worth less than 200 billion dollars at the time of this essay. Highly liquid assets are composed of trillions of dollars in total value which makes it easy to buy or sell any given asset. The world’s six largest companies have a market value three times greater than the market capitalization of all cryptocurrencies.  How about other assets: Gold: 7 trillion.  Stock Markets: 70 trillion.  Global debt: Over 200 trillion.  And the holy grail, derivatives: over 550 trillion. Futures, options, swaps and other contracts between individuals based on the performance of an asset, index, or entity.  So, how does a new asset class attract liquidity? By allowing anyone to create and trade assets that don’t trade anywhere else. The best part is Ethereum literally enables entities or individuals to build new financial instruments and products which will be one of the greatest financial explosions in our lifetime.  One critical aspect is that these protocols are open. UMA is creating a derivatives platform providing standardized contracts for financial products. Synthetix is a protocol that enables the creation and issuance of synthetic assets such as cryptocurrencies, fiat currencies, and commodities. Synthetic assets and derivatives provided by open-source protocols will generate value for investors who seek to hedge against risk, diversify capital allocation, and find mechanisms to increase the return on investments. The types of derivatives are endless for traditional and cryptoassets. There are already over a dozen variations of Dai in existence which have been created by other DeFi protocols.  Staking is increasingly becoming a method of capital deployment. The concept of staking has emerged alongside the growth of cryptonetworks, specifically proof of stake blockchains. Staking is a broad term, but often refers to a mechanism by which individuals are required to support their conviction of an action with capital. Numerai’s stock prediction tournament requires that data scientists stake their predictions with their capital (NMR tokens). The data scientists are then rewarded based on the accuracy of their predictions and on the amount of capital that they staked. The more conviction, the more an individual is willing to stake.  Other staking models for blockchain networks such as Ethereum will require 32 ETH to partake in block validation. If an individual acts in bad faith then their staked capital will be seized by the protocol. Various proof of stake blockchains such as Cosmos and Tezos have other staking models with unique properties including governance (e.g., voting) and rewards.  With the increasing popularity of lending and staking protocols, companies are looking to help individuals manage their capital and staking operations. Coinbase stakes Tezos on behalf of customers and deposits the rewards directly into customer accounts. Many exchanges have started to offer similar products or will in the near future. Another approach to staking investments has been through non-exchange custodians such as Staked and Atlas which provide staking as a service for individuals. While centralized exchanges have the opportunity to offer this feature for their existing users, specialized services may be able to capture a percentage of market share if individuals are more interested in the active governance of a particular crypto network versus simply earning rewards. It’s possible that lending and borrowing protocols like Compound will eventually offer staking as a service features. More exchanges and applications will offer staking services in 2020 because of Ethereum’s shift towards proof of stake as well as the launch of several PoS blockchains.  Risk management and regulatory protections are admittedly less robust than the previous categories within Ethereum and crypto as a whole. In general, Ethereum natively mitigates some of the risk factors of the current financial system that come from opaque information, intermediaries, proprietary systems, and less effective social coordination (e.g., trust). However, Ethereum currently comes with its own set of tradeoffs that result from rapidly developing technology, such as smart contracts that can be designed with errors or manipulated if not created correctly. Further, insurance risk, liquidity risks, and more nuanced features of crypto networks make open finance complex from a risk management perspective.  Consensys Codefi’s DeFi Score has outlined many of these risk factors in more depth. DeFi is a rapidly growing space in Ethereum and the past year brought many of these issues into the limelight as companies, individuals, and developers now seek to minimize these risk factors. Of course, solutions and new business models will help eliminate or at least reduce some of these problems, albeit it may take some time.  From the regulatory side, things are getting better, but it’s mostly a wait and see what the SEC, CFTC, and other global regulatory bodies say about cryptoassets. In 2019, the regulatory clarity for cryptoassets in regards to trading, issuance, and usage has improved and become more defined. However, there are still hundreds of nuanced questions that need answers, and subsequently, dozens of laws that will be required before widespread adoption is possible. The volatile prices of crypto and entrance of large multinational companies like Facebook are consistently bringing cryptoasset regulation centerstage.  Innovative technologies create new or alter existing business models for profit generation and investment.  The internet empowered more efficient crowdfunding through sites like GoFundMe, Kickstarter, and Indiegogo. StockX created a robust marketplace for sneakerheads which increases liquidity and value for collectors.  While not a necessary requirement for a financial system, new markets open up the potential to further manage and hedge against types of risk. Prediction markets and information markets are one instance where thousands of individuals can leverage their knowledge collectively or independently. For instance, Augur’s prediction market is being used to bet on election outcomes which could prove incredibly valuable over time as organizations around the world seek to understand the likelihood of political outcomes.  Numerai’s Erasure protocol provides a data marketplace where individuals can use their specific knowledge to sell predictions or information about the world which are transparent and contractually enforceable.  Another aspect of betting is the lottery, which we all know isn’t worth playing. However, PoolTogether has created a new “no-loss lottery” that leverages other open finance protocols. For example, PoolTogether allows anyone to purchase a lottery ticket in dai and then the proceeds are held in Compound (generating interest) for a certain period of time (week, month, year). The winner of the lottery receives the combined interest that has accrued from all the pooled funds and everyone else (the losers) receives their original money back. Interesting new gambling mechanisms like this may reduce the negative outcomes that individuals receive from the traditional lottery. More importantly, this is once again a perfect example of how open protocols and applications can be leveraged to create something new.   Stablecoins are cryptoassets that maintain a stable value against a target price (e.g., the U.S dollar). There are multiple categories of stablecoins that have emerged over the course of the past two years. MakerDAO has created Dai. Facebook is gearing up for Libra. Walmart is prepping for the Walmart Coin. Coinbase issued USDC, which is available in over 80 countries.  Binance recently announced Venus. JP Morgan created a digital asset for settling transactions between institutional clients. The International Monetary Fund has stated that “Stablecoins are a threat to banking and cash.”  Each of these organizations are taking a specific route to stablecoin adoption. MakerDAO is looking to reshape the financial system and create a more stable cryptocurrency. Facebook is aiming to become a payments company using their social media prowess. JP Morgan and other banks are trying to increase operational efficiency. USDC has been used as a trading pair for cryptoassets and more recently in lending and borrowing protocols. Walmart will likely use its stablecoin as an upgrade to loyalty points. Each of these paths towards mainstream adoption differs slightly and it will be interesting to watch which method proves most effective.  In the short term, fiat-collateralized stablecoins will dominate because they solve various problems with our current system: cost, reach, speed, and openness.  Stablecoins as digital currencies offer near-instantaneous transfers at lower costs anywhere in the world. Digital currencies can be programmed, tracked, and embedded in digital applications. Fiat-collateralized stablecoins can become native to social media and have the potential to reach millions of people that have never had access to traditional financial services. This movement has already been catalyzed by Venmo, WeChat, Apple, and other companies around the world.  Non-fiat collateralized stablecoins will likely take longer for widespread adoption because it requires a mental shift to reimagining value. New forms of money and their adoption will greatly depend on the ability to be used as a store of value and as a means of payment/medium of exchange. Ethereum has become the defacto collateral that backs many of the existing open financial protocols. As more protocols issue new derivatives and securities they may also seek to use Ethereum as collateral further cementing Ethereum at the center of open finance.  It’s clear that Ethereum has established itself as the protocol that will become the foundation for the new financial economy. In 2020, new and existing companies will develop on top of borrowing and lending protocols that will enhance efficiency and build new products. Some of these have already been created, such as Idle and Staked’s robo-advisor for yield generation. More will develop over the course of the next year. Crypto companies are increasing their capabilities to facilitate better trading as well as new products and services. Coinbase and Binance will continue their rapid expansion through acquisitions and product releases. Decentralized exchanges will hopefully become easier for the average user and perhaps mobile native.  There will be an explosion of synthetic assets and new derivatives which will create tens of millions in value in 2020 and eventually billions in value. Some will be shit (literally), while others will be new and novel. New financial products and instruments are continually being developed that will drive further liquidity and potential profits for investors.  Speculation will continue to grow in 2020 as cryptoassets evolve with the addition of newer investing categories like staking, information markets, and yet to be conceived ideas will further create value for individuals, entities, and investors. Risk management will continue to improve with a more comprehensive analysis of the associated risk with open finance from the natural expansion of security-focused entities like MythX and Quantstamp. Moreover, regulatory guidance will gain greater clarity that will hopefully also be favorable for the continued use of cryptoassets and blockchain networks.  The interoperable, programmable, and composable nature of the open financial stack built on Ethereum provides the foundation for a new financial economy. However, remember that this movement is not merely throwing out the old financial system, but rather integrating with the current system where possible and creating where necessary. The new financial system must serve everyone, not only the already crypto-initiated. Lastly, the creation of a new financial system will take time. While 2019 saw significant strides forward in the development of decentralized finance, 2020 looks set to make a leap. Subscribe to Consensys newsletters→  Products Ecosystem Social Company
================================================================================
Title: The Meaning of Decentralization. “Decentralization” is one of the words… | by Vitalik Buterin | Medium
Date: Date not available
Content:
Sign up Sign in Sign up Sign in Vitalik Buterin Follow -- 91 Listen Share “Decentralization” is one of the words that is used in the cryptoeconomics space the most frequently, and is often even viewed as a blockchain’s entire raison d’être, but it is also one of the words that is perhaps defined the most poorly. Thousands of hours of research, and billions of dollars of hashpower, have been spent for the sole purpose of attempting to achieve decentralization, and to protect and improve it, and when discussions get rivalrous it is extremely common for proponents of one protocol (or protocol extension) to claim that the opposing proposals are “centralized” as the ultimate knockdown argument. But there is often a lot of confusion as to what this word actually means. Consider, for example, the following completely unhelpful, but unfortunately all too common, diagram: Now, consider the two answers on Quora for “what is the difference between distributed and decentralized”. The first essentially parrots the above diagram, whereas the second makes the entirely different claim that “distributed means not all the processing of the transactions is done in the same place”, whereas “decentralized means that not one single entity has control over all the processing”. Meanwhile, the top answer on the Ethereum stack exchange gives a very similar diagram, but with the words “decentralized” and “distributed” switched places! Clearly, a clarification is in order. When people talk about software decentralization, there are actually three separate axes of centralization/decentralization that they may be talking about. While in some cases it is difficult to see how you can have one without the other, in general they are quite independent of each other. The axes are as follows: We can try to put these three dimensions into a chart: Note that a lot of these placements are very rough and highly debatable. But let’s try going through any of them: Many times when people talk about the virtues of a blockchain, they describe the convenience benefits of having “one central database”; that centralization is logical centralization, and it’s a kind of centralization that is arguably in many cases good (though Juan Benet from IPFS would also push for logical decentralization wherever possible, because logically decentralized systems tend to be good at surviving network partitions, work well in regions of the world that have poor connectivity, etc; see also this article from Scuttlebot explicitly advocating logical decentralization). Architectural centralization often leads to political centralization, though not necessarily — in a formal democracy, politicians meet and hold votes in some physical governance chamber, but the maintainers of this chamber do not end up deriving any substantial amount of power over decision-making as a result. In computerized systems, architectural but not political decentralization might happen if there is an online community which uses a centralized forum for convenience, but where there is a widely agreed social contract that if the owners of the forum act maliciously then everyone will move to a different forum (communities that are formed around rebellion against what they see as censorship in another forum likely have this property in practice). Logical centralization makes architectural decentralization harder, but not impossible — see how decentralized consensus networks have already been proven to work, but are more difficult than maintaining BitTorrent. And logical centralization makes political decentralization harder — in logically centralized systems, it’s harder to resolve contention by simply agreeing to “live and let live”. The next question is, why is decentralization useful in the first place? There are generally several arguments raised: All three arguments are important and valid, but all three arguments lead to some interesting and different conclusions once you start thinking about protocol decisions with the three individual perspectives in mind. Let us try to expand out each of these arguments one by one. Regarding fault tolerance, the core argument is simple. What’s less likely to happen: one single computer failing, or five out of ten computers all failing at the same time? The principle is uncontroversial, and is used in real life in many situations, including jet engines, backup power generators particularly in places like hospitals, military infrastructure, financial portfolio diversification, and yes, computer networks. However, this kind of decentralization, while still effective and highly important, often turns out to be far less of a panacea than a naive mathematical model would sometimes predict. The reason is common mode failure. Sure, four jet engines are less likely to fail than one jet engine, but what if all four engines were made in the same factory, and a fault was introduced in all four by the same rogue employee? Do blockchains as they are today manage to protect against common mode failure? Not necessarily. Consider the following scenarios: A holistic view of fault tolerance decentralization would look at all of these aspects, and see how they can be minimized. Some natural conclusions that arise are fairly obvious: Note that the fault tolerance requirement in its naive form focuses on architectural decentralization, but once you start thinking about fault tolerance of the community that governs the protocol’s ongoing development, then political decentralization is important too. Now, let’s look at attack resistance. In some pure economic models, you sometimes get the result that decentralization does not even matter. If you create a protocol where the validators are guaranteed to lose $50 million if a 51% attack (ie. finality reversion) happens, then it doesn’t really matter if the validators are controlled by one company or 100 companies — $50 million economic security margin is $50 million economic security margin. In fact, there are deep game-theoretic reasons why centralization may even maximize this notion of economic security (the transaction selection model of existing blockchains reflects this insight, as transaction inclusion into blocks through miners/block proposers is actually a very rapidly rotating dictatorship). However, once you adopt a richer economic model, and particularly one that admits the possibility of coercion (or much milder things like targeted DoS attacks against nodes), decentralization becomes more important. If you threaten one person with death, suddenly $50 million will not matter to them as much anymore. But if the $50 million is spread between ten people, then you have to threaten ten times as many people, and do it all at the same time. In general, the modern world is in many cases characterized by an attack/defense asymmetry in favor of the attacker — a building that costs $10 million to build may cost less than $100,000 to destroy, but the attacker’s leverage is often sublinear: if a building that costs $10 million to build costs $100,000 to destroy, a building that costs $1 million to build may realistically cost perhaps $30,000 to destroy. Smaller gives better ratios. What does this reasoning lead to? First of all, it pushes strongly in favor of proof of stake over proof of work, as computer hardware is easy to detect, regulate, or attack, whereas coins can be much more easily hidden (proof of stake also has strong attack resistance for other reasons). Second, it is a point in favor of having widely distributed development teams, including geographic distribution. Third, it implies that both the economic model and the fault-tolerance model need to be looked at when designing consensus protocols. Finally, we can get to perhaps the most intricate argument of the three, collusion resistance. Collusion is difficult to define; perhaps the only truly valid way to put it is to simply say that collusion is “coordination that we don’t like”. There are many situations in real life where even though having perfect coordination between everyone would be ideal, one sub-group being able to coordinate while the others cannot is dangerous. One simple example is antitrust law — deliberate regulatory barriers that get placed in order to make it more difficult for participants on one side of the marketplace to come together and act like a monopolist and get outsided profits at the expense of both the other side of the marketplace and general social welfare. Another example is rules against active coordination between candidates and super-PACs in the United States, though those have proven difficult to enforce in practice. A much smaller example is a rule in some chess tournaments preventing two players from playing many games against each other to try to raise one player’s score. No matter where you look, attempts to prevent undesired coordination in sophisticated institutions are everywhere. In the case of blockchain protocols, the mathematical and economic reasoning behind the safety of the consensus often relies crucially on the uncoordinated choice model, or the assumption that the game consists of many small actors that make decisions independently. If any one actor gets more than 1/3 of the mining power in a proof of work system, they can gain outsized profits by selfish-mining. However, can we really say that the uncoordinated choice model is realistic when 90% of the Bitcoin network’s mining power is well-coordinated enough to show up together at the same conference? Blockchain advocates also make the point that blockchains are more secure to build on because they can’t just change their rules arbitrarily on a whim whenever they want to, but this case would be difficult to defend if the developers of the software and protocol were all working for one company, were part of one family and sat in one room. The whole point is that these systems should not act like self-interested unitary monopolies. Hence, you can certainly make a case that blockchains would be more secure if they were more discoordinated. However, this presents a fundamental paradox. Many communities, including Ethereum’s, are often praised for having a strong community spirit and being able to coordinate quickly on implementing, releasing and activating a hard fork to fix denial-of-service issues in the protocol within six days. But how can we foster and improve this good kind of coordination, but at the same time prevent “bad coordination” that consists of miners trying to screw everyone else over by repeatedly coordinating 51% attacks? There are three ways to answer this: The first approach makes up a large part of the Casper design philosophy. However, it by itself is insufficient, as relying on economics alone fails to deal with the other two categories of concerns about decentralization. The second is difficult to engineer explicitly, especially for the long term, but it does often happen accidentally. For example, the fact that bitcoin’s core developers generally speak English but miners generally speak Chinese can be viewed as a happy accident, as it creates a kind of “bicameral” governance that makes coordination more difficult, with the side benefit of reducing the risk of common mode failure, as the English and Chinese communities will reason at least somewhat separately due to distance and communication difficulties and are therefore less likely to both make the same mistake. The third is a social challenge more than anything else; solutions in this regard may include: This third kind of decentralization, decentralization as undesired-coordination-avoidance, is thus perhaps the most difficult to achieve, and tradeoffs are unavoidable. Perhaps the best solution may be to rely heavily on the one group that is guaranteed to be fairly decentralized: the protocol’s users. -- -- 91 Help Status About Careers Press Blog Privacy Terms Text to speech Teams
================================================================================
Title: Urban Development Overview
Date: Date not available
Content:
With 189 member countries, staff from more than 170 countries, and offices in over 130 locations, the World Bank Group is a unique global partnership: five institutions working for sustainable solutions that reduce poverty and build shared prosperity in developing countries.   The World Bank Group works in every major area of development.  We provide a wide array of financial products and technical assistance, and we help countries share and apply innovative knowledge and solutions to the challenges they face. We face big challenges to help the world’s poorest people and ensure that everyone sees benefits from economic growth. Data and research help us understand these challenges and set priorities, share knowledge of what works, and measure progress. As the world's largest multilateral financier of urban development, the World Bank works with national and local governments to build more livable, sustainable, and inclusive cities and communities.  Today, some 56% of the world’s population – 4.4 billion inhabitants – live in cities. This trend is expected to continue, with the urban population more than doubling its current size by 2050, at which point nearly 7 of 10 people will live in cities.   With more than 80% of global GDP generated in cities, urbanization can contribute to sustainable growth through increased productivity and innovation if managed well.   However, the speed and scale of urbanization brings challenges, such as meeting accelerated demand for affordable housing, viable infrastructure including transport systems, basic services, and jobs, particularly for the nearly 1 billion urban poor who live in informal settlements to be near opportunities. Rising conflicts contribute to pressure on cities as more than 50% of forcibly displaced people live in urban areas.   Once a city is built, its physical form and land use patterns can be locked in for generations, leading to unsustainable sprawl. The expansion of urban land consumption outpaces population growth by as much as 50%, which is expected to add 1.2 million km² of new urban built-up area to the world by 2030. Such sprawl puts pressure on land and natural resources, resulting in undesirable outcomes; cities represent two-thirds of global energy consumption and account for more than 70% of greenhouse gas emissions.   Cities play an increasingly important role in tackling climate change, because their exposure to climate and disaster risk increases as they grow. Since 1985, about 76,400 km2 of newly urbanized land was added in locations with inundation depths of over 0.5 meters during severe floods – this corresponds to about 50 times the area of Greater London. Globally, 1.81 billion people (that is 1 in 4 people) live in high-risk flood zones. Exposure is especially high in the densely populated and rapidly urbanizing river plains and coastlines in developing countries, where 89% of the world’s flood-exposed people live.    Cities are also in the frontline of combating epidemics. The COVID-19 pandemic was a massive challenge for cities and their citizens, rich and poor alike. Its impact and the measures taken to control the spread of the virus had disproportionate impacts on the poor, marginalized and vulnerable, revealing fault lines in cities’ economic structure, preparedness for such a crisis – especially the state of their public health and service delivery systems.   Building cities that “work” – green, resilient and inclusive– requires intensive policy coordination and investment choices. National and local governments have an important role to play to act now, to shape the future of their development, and to create opportunities for all.  Last Updated: Apr 03,2023 The World Bank’s work in urban development aims to build sustainable cities and communities through an urbanization process that is green, inclusive, competitive, and resilient, contributing to the Sustainable Development Goal (SDG) No.11, to the implementation of the New Urban Agenda, as well as to the World Bank’s goals to end extreme poverty and boost shared prosperity. The World Bank invests an average of $5 billion in planning and implementing lending projects on sustainable cities and communities every year to help cities meet the critical demands of urbanization. The active portfolio stands at 231 projects amounting to $33.9 billion, through a combination of instruments, including investment project financing, policy development loans, and Program-for-Results funding. Specifically, the Bank adopts integrated approaches to transform the fundamental systems of cities, focusing on five priorities, as follows:     1. Enhance Planning System and Local Capacity for cities The first key strategy is to help cities strengthen their planning systems and local capacities to better design, plan, and manage city assets and urban environments. In this context, the Bank provides cities with various diagnostic tools that enable informed planning decisions, as well as investments in urban infrastructure and service delivery.     2. Strengthen Fiscal and Financing Systems  The second strategy aims to maximize multiple financial resources for cities through enhancing fiscal and financial systems, to enable them to meet their infrastructure and service delivery financing and investment needs. Driven by sustained city growth and the need to adapt to and mitigate climate change, financing needs for urban infrastructure at a global level are estimated to be $4.5-5.4 trillion per year, including a 9-27% premium to make this infrastructure low-emission and resilient to climate change impacts. While most of this financing need is likely to be concentrated in developing countries, where historical infrastructure deficits are most pronounced, only a small fraction of the needed financing can be supplied by existing fiscal resources and development partners. Many cities face critical financial constraints to address their infrastructure challenges and make the needed investments. The Bank is well-positioned to help cities expand access to finance from multiple sources, including private finance, but also to strengthen their fiscal capacities and systems that can be sustained in the long run. In addition to providing financing to cities and local governments, the Bank also works with them and national governments to strengthen the basic building blocks of sound city financing by strengthening institutional, fiscal, and regulatory systems. These include: own source revenue generation, including diversifying revenue sources; inter-governmental fiscal transfer systems, especially to regional and local governments in lagging regions which need more financing; improving financial management performance of municipalities and service delivery agencies; and creating an enabling environment for private sector participation and financing through appropriate regulatory frameworks.     3. Promote Territorial and Spatial Development The third key element is to promote territorial development in developing countries and cities. Economic activities are concentrated in only a few places – only 1.5% of the world’s land is home to half of its production. This concentration is inevitable, and it is also desirable. The evidence suggests that prosperous and peaceful countries have been successful by bringing people and businesses closer to each other in cities, harnessing agglomeration economies to boost productivity, job creation, and economic growth. The Bank’s work on territorial development looks at cities not only as individual entities, but also at the coordination between them at different scales: identifying priorities of lagging regions; connecting urban and rural spaces, and addressing spatial inequalities within cities, aiming to allow faster economic growth and linking people to better jobs.     4. Build Climate Smart and Urban resilience With increasingly concentrated people and assets in cities, a complex range of growing shocks and stress imposes tremendous costs on the globe. Disasters disproportionately impact developing countries, which lose an average of 1% of GDP a year to disasters, compared to 0.1-0.3% in developed countries.  Poorer segments of the population are particularly vulnerable, since they tend to live in more hazardous settlements and lack the necessary safety nets to recover from economic or environmental shocks. Without inclusive and climate-informed urban development, climate change can push more than 100 million urban residents back into poverty by 2030.   The Bank focuses on improving cities’ capacity to adapt to a greater variety of changing conditions and to mitigate the impact of climate change through building infrastructure resilience, mobilizing capital, and financing at upstream climate strategy and analysis.     5. Invest in low-income, marginalized urban communities The last key element is fostering more inclusive cities and addressing issues related to urban poverty, slums, and safe and resilient housing solutions for existing and new residents including those relocating due to forced displacement.  For decades, the World Bank has invested in upgrading informal settlement and neighborhoods with improved access to urban infrastructure and services. Targeted analytical work on the urban poor and comprehensive diagnostics mapping out physical, socio-economic and risk profiles of informal settlements have strengthened the World Bank’s advisory role and lending quality in the sector. To address the root causes of informal settlements, the World Bank pursues an integrated approach to the housing sector, operating along the entire value chain, including planning and building regulations, access to land, infrastructure, subsidies and financing, to build safe and inclusive cities.  Capacity building and technical assistance to promote and implement participatory and community-focused development approaches have proven to play a powerful role in addressing acute poverty, inequality and exclusions in cities while also contributing to the process of democratization and empowerment. In particular, the World Bank has mainstreamed and deepened gender inclusion in its urban operations and has promoted disability inclusion in line with leaving no one behind and building accessible cities for all. Given that housing represents about 50-65% of all tangible assets in cities and the construction industry is an important part of many developing countries’ GDP and source of jobs, improving and retrofitting existing housing, building new housing and promoting rental markets in developing countries is critical to activate economic activities and boost economic growth. The Global Program for Resilient Housing was established to respond to the urgency for investments in the identification and improvement of poor-quality housing; and investments in service delivery for at risk urban populations. The five priorities are translated into six business lines and the climate change platform: City management, governance, and finance Land and geospatial Resilience and disaster risk management Sustainable city infrastructure and services Territorial and spatial development Urban poverty, inclusive cities, and housing Cities and climate change Last Updated: Oct 06,2022 Understanding urbanization at different scales: The World Bank is conducting a rich set of research on sustainable urban development. At the regional and country scales, the Urbanization Reviews offer a framework for city leaders to identify policy distortions and analyze investment priorities. A series of prototypes have been piloted to build a body of knowledge on urbanization challenges and public policy implications in a variety of country settings, including Colombia, India, Indonesia, and Vietnam.  Other recent analytical work and tools to help cities manage urbanization and support sustainable, inclusive growth include:   A Review of Integrated Urban Planning Tools for Greenhouse Gas Mitigation : Linking Land Use, Infrastructure Transition, Technology, and Behavioral Change (2020)    Cities, Culture, Creativity : Leveraging Culture and Creativity for Sustainable Urban Development and Inclusive Growth (2021)  Demographic Trends and Urbanization (2021)   Handbook for Gender-Inclusive Urban Planning and Design (2020)   Pancakes to Pyramids : City Form to Promote Sustainable Growth (2021)   Reconsidering Sites and Services : A Global Review, 2022;  Silver Hues : Building Age-Ready Cities (2022)   Transit-Oriented Development Implementation Resources and Tools, 2nd Edition (2021)   Impact of the COVID-19 Pandemic on Municipal Finance (2021)    Subnational Competitiveness Grants Guidebook: A Tool to Promote Jobs and Economic Transformation in Cities and Regions by Leveraging Performance-based Financing (2022)    For decades, the World Bank and the Global Facility for Disaster Reduction and Recovery (GFDRR) have been helping national and local governments to prepare for and mitigate the impacts of naturally occurring events – floods, droughts, cyclones, earthquakes, tsunamis, and more – investing around $5 billion in disaster risk management, on average, every year.  Large systemic shocks — like pandemics or natural hazards — highlight the urgent need for resilient health systems for mitigating the loss of lives and prolonged impacts on people’s livelihoods. The effective response depends not only on the health care system itself, but also on its supporting lifeline infrastructure. GFDRR is contributing to strengthening the resilience of health systems and their enabling environment to a wide range of emergencies, through knowledge such as the Frontline report, tools, and operational analytics.     The World Bank helps cities and national governments put in place the financial framework to attract investment and grow in a sustainable manner. The Bank is helping countries establish and strengthen urban institutions to deliver improved infrastructure and services, for example:   We support cities and local governments across the world to improve their institutional and service delivery performance through performance-based fiscal transfers. In addition to financing, we provide substantial technical assistance and advisory support to cities to improve their ability to design and develop investable projects, improve their creditworthiness, and raise financing for climate and disaster resilience.   Through the capital raising strategy of its City Resilience Program (CRP), the World Bank is pushing the boundaries in this area with its Capital Mobilization Strategy, which works with city leaders to consider the Bank a catalyst to development financial solutions beyond World Bank loans.   The City Creditworthiness Initiative (CCI) aims to strengthen the financial performance of local governments and prepare them to tap domestic / regional capital markets without a sovereign guarantee. The initiative has trained over 630 municipal officials from 250 cities in 26 countries.    In recent years, the World Bank has worked in cities and towns across over 140 countries, investing around $4 billion in FY21 in disaster risk management, to improve their urban infrastructure and systems especially focused on climate and disaster resilience.   In Niger, the Bank is supporting cities build resilience to climate shocks, address flood risk and develop better urban and disaster risk management systems, through a $250 million project.   Building on lessons from previous typhoons such as Typhoon Haiyan (2013), the Philippines together with the World Bank designed a capacity building program Ready to Rebuild (R2R) for governors, mayors, disaster risk management officers, planners, and budget officers. Funded by GFDRR and the Japan-World Bank Program for Mainstreaming Disaster Risk Management in Developing Countries, The R2R Program provides the “how” — how to prepare people, communities, and local governments to be more resilient and ready to respond and recover from disasters better and faster. It provides tools, ready templates, and practical solutions to address disaster recovery issues and bottlenecks.   Urban resilience goes hand-in-hand with environmental sustainability. The World Bank’s Global Platform for Sustainable Cities (GPSC) is a partnership and knowledge platform that promotes integrated solutions and cutting-edge support for cities seeking to improve urban sustainability, build resilience and tackle two global crises: climate change and biodiversity loss.    The City Climate Finance Gap Fund (Gap Fund) is a multi-donor initiative that aims to help cities in developing and emerging countries realize their climate ambitions by turning low-carbon, climate-resilient ideas into strategies and finance-ready projects. The Gap Fund also strengthens knowledge generation for city climate action through its analytical work (Primer on global urban carbon emissions – data sources and trends, The Analysis of Climate Action Plans in Latin America and the Caribbean, etc.) and organizing knowledge exchange.    Last Updated: Apr 03,2023 Find out what the Bank Group's branches are doing on urban development.
 Find out what the Bank Group's branches are doing on urban development. 
                                SURGE is a World Bank global umbrella program that supports developing countries to foster green, resilient and inclusive urban and regional development.
                             
                                TDLC is a partnership between the World Bank and the Government of Japan that maximizes the impact of urban development projects by leveraging Japanese and global knowledge, insights, and technical expertise.
                             
                                The Gap Fund, operationalized by the World Bank and European Investment Bank, helps cities in developing and emerging countries to turn low-carbon, climate-resilient ideas into strategies and finance-ready projects.
                             
                                Led by the World Bank, the Global Platform for Sustainable Cities is a forum for knowledge sharing and partnership to achieve urban sustainability.
                             
                                GSCP supports World Bank teams and cities to integrate data and technology into urban infrastructure and service delivery and provide solutions to achieve a citizen-centric approach.
                             
                                The World Bank's City Creditworthiness Initiative supports local government leaders in building access to long-term financing for investment.
                             
                                City Planning Labs is a World Bank technical assistance program which aims to enhance the capacity of municipal governments to produce, share, and utilize geospatial data for evidence-led urban planning.
                             This site uses cookies to optimize functionality and give you the best possible experience. If you continue to navigate this website beyond this page, cookies will be placed on your browser. To learn more about cookies, click here. 
================================================================================
Title: Gini coefficient - Wikipedia
Date: Date not available
Content:

 In economics, the Gini coefficient (/ˈdʒiːni/ JEE-nee), also known as the Gini index or Gini ratio, is a measure of statistical dispersion intended to represent the income inequality, the wealth inequality, or the consumption inequality[3] within a nation or a social group. It was developed by Italian statistician and sociologist Corrado Gini.
 The Gini coefficient measures the inequality among the values of a frequency distribution, such as levels of income. A Gini coefficient of 0 reflects perfect equality, where all income or wealth values are the same, while a Gini coefficient of 1 (or 100%) reflects maximal inequality among values, a situation where a single individual has all the income while all others have none.[4][5]
 The Gini coefficient was proposed by Corrado Gini as a measure of inequality of income or wealth.[6] For OECD countries in the late 20th century, considering the effect of taxes and transfer payments, the income Gini coefficient ranged between 0.24 and 0.49, with Slovakia being the lowest and Mexico the highest.[7] African countries had the highest pre-tax Gini coefficients in 2008–2009, with South Africa having the world's highest, estimated to be 0.63 to 0.7.[8][9] However, this figure drops to 0.52 after social assistance is taken into account, and drops again to 0.47 after taxation.[10] The country with the lowest Gini coefficient is Slovakia, with a Gini coefficient of 0.232.[11] The Gini coefficient of the global income in 2005 has been estimated to be between 0.61 and 0.68 by various sources.[12][13]
 There are some issues in interpreting a Gini coefficient, as the same value may result from many different distribution curves. To mitigate this, the demographic structure should be taken into account. Countries with an aging population, or those with an increased birth rate, experience an increasing pre-tax Gini coefficient even if real income distribution for working adults remains constant. Many scholars have devised over a dozen variants of the Gini coefficient.[14][15][16]
 The Gini coefficient was developed by the Italian statistician Corrado Gini and published in his 1912 paper Variabilità e mutabilità (English: variability and mutability).[17][18] Building on the work of American economist Max Lorenz, Gini proposed that the difference between the hypothetical straight line depicting perfect equality, and the actual line depicting people's incomes, be used as a measure of inequality.[19] In this paper, he introduced the concept of simple mean difference as a measure of variability.
 He then applied the simple mean difference of observed variables to income and wealth inequality in his work On the measurement of concentration and variability of characters in 1914. Here, he presented the concentration ratio, which further developed in the Gini coefficient used today. Secondly, Gini observed that his proposed ratio can be also achieved by improving methods already introduced by Lorenz, Chatelain, or Séailles.
 In 1915, Gaetano Pietra introduced a geometrical interpretation between Gini's proposed ratio and the ratio between the area of observed concentration and maximum concentration. This altered version of Gini coefficient became the most commonly used inequality index in upcoming years.[20]
 According to data from OECD, Gini coefficient was first officially used country-wide in Canada in the 1970s. Canadian index of income inequality ranged from 0.303 to 0.284 from 1976 to the end of 1980s. OECD started to publish more countries’ data since the start of the 21st century. Central European countries Slovenia, Czechia, and Slovakia have had the lowest inequality index out of all OECD countries ever since the 2000s. Scandinavian countries also frequently appeared at the top of the list of equality in the last decades.[21]
 The Gini coefficient is an index for the degree of inequality in the distribution of income/wealth, used to estimate how far a country's wealth or income distribution deviates from an equal distribution.[22]
 The Gini coefficient is usually defined mathematically based on the Lorenz curve, which plots the proportion of the total income of the population (y-axis) that is cumulatively earned by the bottom x of the population (see diagram).[23] The line at 45 degrees thus represents perfect equality of incomes. The Gini coefficient can then be thought of as the ratio of the area that lies between the line of equality and the Lorenz curve (marked A in the diagram) over the total area under the line of equality (marked A and B in the diagram); i.e., G = A/(A + B). If there are no negative incomes, it is also equal to 2A and 1 − 2B due to the fact that A + B = 0.5.[24]
 Assuming non-negative income or wealth for all, the Gini coefficient's theoretical range is from 0 (total equality) to 1 (absolute inequality). This measure is often rendered as a percentage, spanning 0 to 100. However, if negative values are factored in, as in cases of debt, the Gini index could exceed 1. Typically, we presuppose a positive mean or total, precluding a Gini coefficient below zero.[25]
 An alternative approach is to define the Gini coefficient as half of the relative mean absolute difference, which is equivalent to the definition based on the Lorenz curve.[26]  The mean absolute difference is the average absolute difference of all pairs of items of the population, and the relative mean absolute difference is the mean absolute difference divided by the average, 






x
¯





{\displaystyle {\bar {x}}}

, to normalize for scale. If xi is the wealth or income of person i, and there are n persons, then the Gini coefficient G is given by:
 When the income (or wealth) distribution is given as a continuous probability density function p(x), the Gini coefficient is again half of the relative mean absolute difference:
 where 




μ
=

∫

−
∞


∞


x
p
(
x
)

d
x



{\displaystyle \textstyle \mu =\int _{-\infty }^{\infty }xp(x)\,dx}

 is the mean of the distribution, and the lower limits of integration may be replaced by zero when all incomes are positive.[27]
 While the income distribution of any particular country will not correspond perfectly to the theoretical models,  these models can provide a qualitative explanation of the income distribution in a nation given the Gini coefficient.
 The extreme cases are represented by the most equal possible society in which every person receives the same income (G = 0), and the most unequal society (with N individuals) where a single person receives 100% of the total income and the remaining N − 1 people receive none (G = 1 − 1/N).
 A simple case assumes just two levels of income, low and high. If the high income group is a proportion u of the population and earns a proportion f of all income, then the Gini coefficient is f − u. A more graded distribution with these same values u and f will always have a higher Gini coefficient than f − u.
 For example, if the wealthiest u = 20% of the population has f = 80% of all income (see Pareto principle), the income Gini coefficient is at least 60%. In another example,[28] if u = 1% of the world's population owns  f = 50% of all wealth, the wealth Gini coefficient is at least 49%.
 In some cases, this equation can be applied to calculate the Gini coefficient without direct reference to the Lorenz curve. For example, (taking y to indicate the income or wealth of a person or household):
 The Gini coefficient can also be considered as half the relative mean absolute difference. For a random sample S with values 




y

1


≤

y

2


≤
⋯
≤

y

n




{\displaystyle y_{1}\leq y_{2}\leq \cdots \leq y_{n}}

, the sample Gini coefficient
 is a consistent estimator of the population Gini coefficient, but is not in general unbiased. In simplified form:
 There does not exist a sample statistic that is always an unbiased estimator of the population Gini coefficient.
 For a discrete probability distribution with probability mass function 



f
(

y

i


)
,


{\displaystyle f(y_{i}),}

 



i
=
1
,
…
,
n


{\displaystyle i=1,\ldots ,n}

, where  



f
(

y

i


)


{\displaystyle f(y_{i})}

 is the fraction of the population with income or wealth 




y

i


>
0


{\displaystyle y_{i}>0}

, the Gini coefficient is:
 where
 If the points with non-zero probabilities are indexed in increasing order 



(

y

i


<

y

i
+
1


)


{\displaystyle (y_{i}<y_{i+1})}

, then:
 where
 When the population is large, the income distribution may be represented by a continuous probability density function f(x) where f(x) dx is the fraction of the population with wealth or income in the interval dx about x.  If F(x) is the cumulative distribution function for f(x):
 and L(x) is the Lorenz function:
 then the Lorenz curve L(F) may then be represented as a function parametric in L(x) and F(x) and the value of B can be found by integration:
 The Gini coefficient can also be calculated directly from the cumulative distribution function of the distribution F(y). Defining μ as the mean of the distribution, and specifying that F(y) is zero for all negative values, the Gini coefficient is given by:
 The latter result comes from integration by parts. (Note that this formula can be applied when there are negative values if the integration is taken from minus infinity to plus infinity.)
 The Gini coefficient may be expressed in terms of the quantile function Q(F) (inverse of the cumulative distribution function: Q(F(x)) = x)
 Since the Gini coefficient is independent of scale, if the distribution function can be expressed in the form f(x,φ,a,b,c...) where φ is a scale factor and a, b, c... are dimensionless parameters, then the Gini coefficient will be a function only of a, b, c....[30] For example, for the exponential distribution, which is a function of only x and a scale parameter, the Gini coefficient is a constant, equal to 1/2.
 For some functional forms, the Gini index can be calculated explicitly. For example, if y follows a log-normal distribution with the standard deviation of logs equal to 



σ


{\displaystyle \sigma }

, then 



G
=
erf
⁡

(


σ
2


)



{\displaystyle G=\operatorname {erf} \left({\frac {\sigma }{2}}\right)}

 where 



erf


{\displaystyle \operatorname {erf} }

 is the error function ( since 



G
=
2
Φ

(


σ

2



)

−
1


{\displaystyle G=2\Phi \left({\frac {\sigma }{\sqrt {2}}}\right)-1}

, where  



Φ


{\displaystyle \Phi }

 is the cumulative distribution function of a standard normal distribution).[31]  In the table below, some examples for probability density functions with support on 



[
0
,
∞
)


{\displaystyle [0,\infty )}

 are shown. The Dirac delta distribution represents the case where everyone has the same wealth (or income); it implies no variations between incomes.[32]
 Sometimes the entire Lorenz curve is not known, and only values at certain intervals are given. In that case, the Gini coefficient can be approximated using various techniques for interpolating the missing values of the Lorenz curve. If (Xk, Yk) are the known points on the Lorenz curve, with the Xk indexed in increasing order (Xk – 1 < Xk), so that:
 If the Lorenz curve is approximated on each interval as a line between consecutive points, then the area B can be approximated with trapezoids and:
 is the resulting approximation for G. More accurate results can be obtained using other methods to approximate the area B, such as approximating the Lorenz curve with a quadratic function across pairs of intervals or building an appropriately smooth approximation to the underlying distribution function that matches the known data. If the population mean and boundary values for each interval are also known, these can also often be used to improve the accuracy of the approximation.
 The Gini coefficient calculated from a sample is a statistic, and its standard error, or confidence intervals for the population Gini coefficient, should be reported. These can be calculated using bootstrap techniques, mathematically complicated and computationally demanding even in an era of fast computers.[41] Economist Tomson Ogwang made the process more efficient by setting up a "trick regression model" in which respective income variables in the sample are ranked, with the lowest income being allocated rank 1. The model then expresses the rank (dependent variable) as the sum of a constant A and a normal error term whose variance is inversely proportional to yk:
 Thus, G can be expressed as a function of the weighted least squares estimate of the constant A and that this can be used to speed up the calculation of the jackknife estimate for the standard error. Economist David Giles argued that the standard error of the estimate of A can be used to derive the estimate of G directly without using a jackknife. This method only requires using ordinary least squares regression after ordering the sample data. The results compare favorably with the estimates from the jackknife with agreement improving with increasing sample size.[42]
 However, it has been argued that this depends on the model's assumptions about the error distributions and the independence of error terms. These assumptions are often not valid for real data sets. There is still ongoing debate surrounding this topic.
 Guillermina Jasso[43] and Angus Deaton[44] independently proposed the following formula for the Gini coefficient:
 where 



μ


{\displaystyle \mu }

 is mean income of the population, Pi is the income rank P of person i, with income X, such that the richest person receives a rank of 1 and the poorest a rank of N. This effectively gives higher weight to poorer people in the income distribution, which allows the Gini to meet the Transfer Principle. Note that the Jasso-Deaton formula rescales the coefficient so that its value is one if all the 




X

i




{\displaystyle X_{i}}

 are zero except one. Note however Allison's reply on the need to divide by N² instead.[45]
 FAO explains another version of the formula.[46]
 The Gini coefficient and other standard inequality indices reduce to a common form. Perfect equality—the absence of inequality—exists when and only when the inequality ratio, 




r

j


=

x

j



/



x
¯




{\displaystyle r_{j}=x_{j}/{\overline {x}}}

, equals 1 for all j units in some population (for example, there is perfect income equality when everyone's income 




x

j




{\displaystyle x_{j}}

 equals the mean income 





x
¯




{\displaystyle {\overline {x}}}

, so that 




r

j


=
1


{\displaystyle r_{j}=1}

 for everyone). Measures of inequality, then, are measures of the average deviations of the 




r

j


=
1


{\displaystyle r_{j}=1}

 from 1; the greater the average deviation, the greater the inequality. Based on these observations the inequality indices have this common form:[47]
 where pj weights the units by their population share, and f(rj) is a function of the deviation of each unit's rj from 1, the point of equality. The insight of this generalized inequality index is that inequality indices differ because they employ different functions of the distance of the inequality ratios (the rj) from 1.
 Gini coefficients of income are calculated on a market income and a disposable income basis. The Gini coefficient on market income—sometimes referred to as a pre-tax Gini coefficient—is calculated on income before taxes and transfers. It measures inequality in income without considering the effect of taxes and social spending already in place in a country. The Gini coefficient on disposable income—sometimes referred to as the after-tax Gini coefficient—is calculated on income after taxes and transfers. It measures inequality in income after considering the effect of taxes and social spending already in place in a country.[7][48][49]
 For OECD countries over the 2008–2009 period, the Gini coefficient (pre-taxes and transfers) for a total population ranged between 0.34 and 0.53, with South Korea the lowest and Italy the highest. The Gini coefficient (after-taxes and transfers) for a total population ranged between 0.25 and 0.48, with Denmark the lowest and Mexico the highest. For the United States, the country with the largest population among OECD countries, the pre-tax Gini index was 0.49, and the after-tax Gini index was 0.38 in 2008–2009. The OECD average for total populations in OECD countries was 0.46 for the pre-tax income Gini index and 0.31 for the after-tax income Gini index.[7][50] Taxes and social spending that were in place in 2008–2009 period in OECD countries significantly lowered effective income inequality, and in general, "European countries—especially Nordic and Continental welfare states—achieve lower levels of income inequality than other countries."[51]
 Using the Gini can help quantify differences in welfare and compensation policies and philosophies. However, it should be borne in mind that the Gini coefficient can be misleading when used to make political comparisons between large and small countries or those with different immigration policies (see limitations section).
 The Gini coefficient for the entire world has been estimated by various parties to be between 0.61 and 0.68.[12][13][52] The graph shows the values expressed as a percentage in their historical development for a number of countries.
 According to UNICEF, Latin America and the Caribbean region had the highest net income Gini index in the world at 48.3, on an unweighted average basis in 2008. The remaining regional averages were: sub-Saharan Africa (44.2), Asia (40.4), Middle East and North Africa (39.2), Eastern Europe and Central Asia (35.4), and High-income Countries (30.9). Using the same method, the United States is claimed to have a Gini index of 36, while South Africa had the highest income Gini index score of 67.8.[53]
 Taking income distribution of all human beings, worldwide income inequality has been constantly increasing since the early 19th century (and will keep on increasing over the years) . There was a steady increase in the global income inequality Gini score from 1820 to 2002, with a significant increase between 1980 and 2002. This trend appears to have peaked and begun a reversal with rapid economic growth in emerging economies, particularly in the large populations of BRIC countries.[54]
 The table below presents the estimated world income Gini coefficients over the last 200 years, as calculated by Milanovic.[55]
 More detailed data from similar sources plots a continuous decline since 1988.  This is attributed to globalization increasing incomes for billions of poor people, mostly in countries like China and India.  Developing countries like Brazil have also improved basic services like health care, education, and sanitation; others like Chile and Mexico have enacted more progressive tax policies.[57]
 The Gini coefficient is widely used in fields as diverse as sociology, economics, health science, ecology, engineering, and agriculture.[59] For example, in social sciences and economics, in addition to income Gini coefficients, scholars have published education Gini coefficients and opportunity Gini coefficients.
 Education Gini index estimates the inequality in education for a given population.[60] It is used to discern trends in social development through educational attainment over time. A study across 85 countries by three World Bank economists, Vinod Thomas, Yan Wang, and Xibo Fan, estimated Mali had the highest education Gini index of 0.92 in 1990 (implying very high inequality in educational attainment across the population), while the United States had the lowest education inequality Gini index of 0.14. Between 1960 and 1990, China, India and South Korea had the fastest drop in education inequality Gini Index. They also claim education Gini index for the United States slightly increased over the 1980–1990 period.
 Though India's education Gini Index has been falling from 1960 through 1990, most of the population still has not received any education, while 10 percent of the population received more than 40% of the total educational hours in the nation. This means that a large portion of capable children in the country are not receiving the support necessary to allow them to become positive contributors to society. This will lead to a deadweight loss to the national society because there are many people who are underdeveloped and underutilized.[61]
 Similar in concept to the Gini income coefficient, the Gini opportunity coefficient measures inequality in opportunities.[62][63][64] The concept builds on Amartya Sen's suggestion[65] that inequality coefficients of social development should be premised on the process of enlarging people's choices and enhancing their capabilities, rather than on the process of reducing income inequality. Kovacevic, in a review of the Gini opportunity coefficient, explained that the coefficient estimates how well a society enables its citizens to achieve success in life where the success is based on a person's choices, efforts and talents, not their background defined by a set of predetermined circumstances at birth, such as gender, race, place of birth, parent's income and circumstances beyond the control of that individual.
 In 2003, Roemer[62][66] reported Italy and Spain exhibited the largest opportunity inequality Gini index amongst advanced economies.
 In 1978, Anthony Shorrocks introduced a measure based on income Gini coefficients to estimate income mobility.[67] This measure, generalized by Maasoumi and Zandvakili,[68] is now generally referred to as Shorrocks index, sometimes as Shorrocks mobility index or Shorrocks rigidity index. It attempts to estimate whether the income inequality Gini coefficient is permanent or temporary and to what extent a country or region enables economic mobility to its people so that they can move from one (e.g., bottom 20%) income quantile to another (e.g., middle 20%) over time. In other words, the Shorrocks index compares inequality of short-term earnings, such as the annual income of households, to inequality of long-term earnings, such as 5-year or 10-year total income for the same households.
 Shorrocks index is calculated in several different ways, a common approach being from the ratio of income Gini coefficients between short-term and long-term for the same region or country.[69]
 A 2010 study using social security income data for the United States since 1937 and Gini-based Shorrock's indices concludes that income mobility in the United States has had a complicated history, primarily due to the mass influx of women into the American labor force after World War II. Income inequality and income mobility trends have been different for men and women workers between 1937 and the 2000s. When men and women are considered together, the Gini coefficient-based Shorrocks index trends imply long-term income inequality has been substantially reduced among all workers, in recent decades for the United States.[69] Other scholars, using just 1990s data or other short periods have come to different conclusions.[70] For example, Sastre and Ayala conclude from their study of income Gini coefficient data between 1993 and 1998 for six developed economies that France had the least income mobility, Italy the highest, and the United States and Germany intermediate levels of income mobility over those five years.[71]
 The Gini coefficient has features that make it useful as a measure of dispersion in a population, and inequalities in particular.[46] The coefficient ranges from 0, for perfect equality, to 1, indicating perfect inequality. The Gini is based on the comparison of cumulative proportions of the population against cumulative proportions of income they receive.[72]
 The Gini coefficient is a relative measure. The Gini coefficient of a developing country can rise (due to increasing inequality of income) even when the number of people in absolute poverty decreases.[73] This is because the Gini coefficient measures relative, not absolute, wealth.
 Gini coefficients are simple, and this simplicity can lead to oversights and can confuse the comparison of different populations; for example, while both Bangladesh (per capita income of $1,693) and the Netherlands (per capita income of $42,183) had an income Gini coefficient of 0.31 in 2010,[74] the quality of life, economic opportunity and absolute income in these countries are very different, i.e. countries may have identical Gini coefficients, but differ greatly in wealth. Basic necessities may be available to all in a developed economy, while in an undeveloped economy with the same Gini coefficient, basic necessities may be unavailable to most or unequally available due to lower absolute wealth.
 Gini has some mathematical limitations as well. It is not additive and different sets of people cannot be averaged to obtain the Gini coefficient of all the people in the sets.
 Even when the total income of a population is the same, in certain situations two countries with different income distributions can have the same Gini index (e.g. cases when income Lorenz Curves cross).[46] Table A illustrates one such situation. Both countries have a Gini coefficient of 0.2, but the average income distributions for household groups are different. As another example, in a population where the lowest 50% of individuals have no income, and the other 50% have equal income, the Gini coefficient is 0.5; whereas for another population where the lowest 75% of people have 25% of income and the top 25% have 75% of the income, the Gini index is also 0.5. Economies with similar incomes and Gini coefficients can have very different income distributions. Bellù and Liberati claim that ranking income inequality between two populations is not always possible based on their Gini indices.[75] Similarly, computational social scientist Fabian Stephany illustrates that income inequality within the population, e.g., in specific socioeconomic groups of same age and education, also remains undetected by conventional Gini indices.[76]
 A Gini index does not contain information about absolute national or personal incomes. Populations can simultaneously have very low income Gini indices and very high wealth Gini indexes. By measuring inequality in income, the Gini ignores the differential efficiency of the use of household income. By ignoring wealth (except as it contributes to income), the Gini can create the appearance of inequality when the people compared are at different stages in their life. Wealthy countries such as Sweden can show a low Gini coefficient for the disposable income of 0.31, thereby appearing equal, yet have a very high Gini coefficient for wealth of 0.79 to 0.86, suggesting an extremely unequal wealth distribution in its society.[77][78] These factors are not assessed in income-based Gini.
 Gini index has a downward-bias for small populations.[79] Counties or states or countries with small populations and less diverse economies will tend to report small Gini coefficients. For economically diverse large population groups, a much higher coefficient is expected than for each of its regions. For example, taking the world economy as a whole and income distribution for all human beings, different scholars estimate the global Gini index to range between 0.61 and 0.68.[12][13]
As with other inequality coefficients, the Gini coefficient is influenced by the granularity of the measurements. For example, five 20% quantiles (low granularity) will usually yield a lower Gini coefficient than twenty 5% quantiles (high granularity) for the same distribution. Philippe Monfort has shown that using inconsistent or unspecified granularity limits the usefulness of Gini coefficient measurements.[80]
 Changing income inequality, measured by Gini coefficients, can be due to structural changes in a society such as growing population (increased birth rates, aging populations, emigration, immigration) and income mobility.[81]
 Another limitation of the Gini coefficient is that it is not a proper measure of egalitarianism, as it only measures income dispersion. For example, suppose two equally egalitarian countries pursue different immigration policies. In that case, the country accepting a higher proportion of low-income or impoverished migrants will report a higher Gini coefficient and, therefore, may exhibit more income inequality.
 The Gini coefficient measure gives different results when applied to individuals instead of households, for the same economy and same income distributions. If household data is used, the measured value of income Gini depends on how the household is defined. The comparison is not meaningful when different populations are not measured with consistent definitions. Furthermore, changes to the household income Gini can be driven by changes in household formation, such as increased divorce rates or extended family households splitting into nuclear families.
 Deininger and Squire (1996) show that the income Gini coefficient based on individual income rather than household income is different. For example, for the United States, they found that the individual income-based Gini index was 0.35, while for France, 0.43. According to their individual-focused method, in the 108 countries they studied, South Africa had the world's highest Gini coefficient at 0.62, Malaysia had Asia's highest Gini coefficient at 0.5, Brazil the highest at 0.57 in Latin America and the Caribbean region, and Turkey the highest at 0.5 in OECD countries.[82]
 Billionaire Thomas Kwok claimed the income Gini coefficient for Hong Kong has been high (0.434 in 2010[74]), in part because of structural changes in its population. Over recent decades, Hong Kong has witnessed increasing numbers of small households, elderly households, and elderly living alone. The combined income is now split into more households. Many older people live separately from their children in Hong Kong. These social changes have caused substantial changes in household income distribution. The income Gini coefficient, claims Kwok, does not discern these structural changes in its society.[81] Household money income distribution for the United States, summarized in Table C of this section, confirms that this issue is not limited to just Hong Kong. According to the US Census Bureau, between 1979 and 2010, the population of the United States experienced structural changes in overall households; the income for all income brackets increased in inflation-adjusted terms, household income distributions shifted into higher income brackets over time, while the income Gini coefficient increased.[83][84]
 The Gini coefficient is unable to discern the effects of structural changes in populations.[81] Expanding on the importance of life-span measures, the Gini coefficient as a point-estimate of equality at a certain time ignores life-span changes in income. Typically, increases in the proportion of young or old members of a society will drive apparent changes in equality simply because people generally have lower incomes and wealth when they are young than when they are old. Because of this, factors such as age distribution within a population and mobility within income classes can create the appearance of inequality when none exist, taking into account demographic effects. Thus a given economy may have a higher Gini coefficient at any timepoint compared to another, while the Gini coefficient calculated over individuals' lifetime income is lower than the apparently more equal (at a given point in time) economy's.[clarification needed][16] Essentially, what matters is not just inequality in any particular year but the distribution composition over time.
 Inaccuracies in assign monetary value to income in kind reduce the accuracy of Gini as a measurement of true inequality.
 While taxes and cash transfers are relatively straightforward to account for, other government benefits can be difficult to value. Benefits such as subsidized housing, medical care, and education are difficult to value objectively, as it depends on the quality and extent of the benefit. In absence of a free market, valuing these income transfers as household income is subjective. The theoretical model of the Gini coefficient is limited to accepting correct or incorrect subjective assumptions.
 In subsistence-driven and informal economies, people may have significant income in other forms than money, for example, through subsistence farming or bartering. These forms of income tend to accrue to poor segments of populations in emerging and transitional economy countries such as those in sub-Saharan Africa, Latin America, Asia, and Eastern Europe. Informal economy accounts for over half of global employment and as much as 90 percent of employment in some of the poorer sub-Saharan countries with high official Gini inequality coefficients. Schneider et al., in their 2010 study of 162 countries,[85] report about 31.2%, or about $20 trillion, of world's GDP is informal. In developing countries, the informal economy predominates for all income brackets except the richer, urban upper-income bracket populations. Even in developed economies, 8% (United States) to 27% (Italy) of each nation's GDP is informal. The resulting informal income predominates as a livelihood activity for those in the lowest income brackets.[86] The value and distribution of the incomes from informal or underground economy is difficult to quantify, making true income Gini coefficients estimates difficult.[87][88] Different assumptions and quantifications of these incomes will yield different Gini coefficients.[89][90][91]
 Given the limitations of the Gini coefficient, other statistical methods are used in combination or as an alternative measure of population dispersity. For example, entropy measures are frequently used (e.g. the Atkinson index or the Theil Index and Mean log deviation as special cases of the generalized entropy index). These measures attempt to compare the distribution of resources by intelligent agents in the market with a maximum entropy random distribution, which would occur if these agents acted like non-interacting particles in a closed system following the laws of statistical physics.
 There is a summary measure of the diagnostic ability of a binary classifier system that is also called the Gini coefficient, which is defined as twice the area between the receiver operating characteristic (ROC) curve and its diagonal.  It is related to the AUC (Area Under the ROC Curve) measure of performance given by 



A
U
C
=
(
G
+
1
)

/

2


{\displaystyle AUC=(G+1)/2}

[92]   and to Mann–Whitney U.  Although both Gini coefficients are defined as areas between certain curves and share certain properties, there is no simple direct relationship between the Gini coefficient of statistical dispersion and the Gini coefficient of a classifier.
 The Gini index is also related to the Pietra index — both of which measure statistical heterogeneity and are derived from the Lorenz curve and the diagonal line.[93][94][30]
 In certain fields such as ecology, inverse Simpson's index 



1

/

λ


{\displaystyle 1/\lambda }

 is used to quantify diversity, and this should not be confused with the Simpson index 



λ


{\displaystyle \lambda }

. These indicators are related to Gini. The inverse Simpson index increases with diversity, unlike the Simpson index and Gini coefficient, which decrease with diversity. The Simpson index is in the range [0, 1], where 0 means maximum and 1 means minimum diversity (or heterogeneity). Since diversity indices typically increase with increasing heterogeneity, the Simpson index is often transformed into inverse Simpson, or using the complement 



1
−
λ


{\displaystyle 1-\lambda }

, known as the Gini-Simpson Index.[95]
 The Lorenz curve is another method of graphical representation of wealth distribution. It was developed 9 years before the Gini coefficient, which quantifies the extent to which the Lorenz curve deviates from the perfect equality line (with slope of 1). The Hoover index (also known as Robin Hood index) presents the percentage of total population's income that would have to be redistributed to make the Gini coefficient equal to 0 (perfect equality).[96]
 In recent decades, researchers have attempted to estimate Gini coefficients for pre-20th century societies. In the absence of household income surveys and income taxes, scholars have relied on proxy variables. These include wealth taxes in medieval European city states, patterns of landownership in Roman Egypt, variation of the size of houses in societies from ancient Greece to Aztec Mexico, and inheritance and dowries in Babylonian society. Other data does not directly document variations in wealth or income but are known to reflect inequality, such as the ratio of rents to wages or of labor to capital.[97]
 Although the Gini coefficient is most popular in economics, it can, in theory, be applied in any field of science that studies a distribution. For example, in ecology, the Gini coefficient has been used as a measure of biodiversity, where the cumulative proportion of species is plotted against the cumulative proportion of individuals.[98] In health, it has been used as a measure of the inequality of health-related quality of life in a population.[99] In education, it has been used as a measure of the inequality of universities.[100] In chemistry it has been used to express the selectivity of protein kinase inhibitors against a panel of kinases.[101] In engineering, it has been used to evaluate the fairness achieved by Internet routers in scheduling packet transmissions from different flows of traffic.[102]
 The Gini coefficient is sometimes used for the measurement of the discriminatory power of rating systems in credit risk management.[103]
 A 2005 study accessed US census data to measure home computer ownership and used the Gini coefficient to measure inequalities amongst whites and African Americans. Results indicated that although decreasing overall, home computer ownership inequality was substantially smaller among white households.[104]
 A 2016 peer-reviewed study titled Employing the Gini coefficient to measure participation inequality in treatment-focused Digital Health Social Networks[105] illustrated that the Gini coefficient was helpful and accurate in measuring shifts in inequality, however as a standalone metric it failed to incorporate overall network size.
 Discriminatory power refers to a credit risk model's ability to differentiate between defaulting and non-defaulting clients. The formula 




G

1




{\displaystyle G_{1}}

, in the calculation section above, may be used for the final model and at the individual model factor level to quantify the discriminatory power of individual factors. It is related to the accuracy ratio in population assessment models.
 The Gini coefficient has also been applied to analyze inequality in dating apps.[106][107]
 Kaminskiy and Krivtsov[108] extended the concept of the Gini coefficient from economics to reliability theory and proposed a Gini-type coefficient that helps to assess the degree of aging of non-repairable systems or aging and rejuvenation of repairable systems.  The coefficient is defined between −1 and 1 and can be used in both empirical and parametric life distributions. It takes negative values for the class of decreasing failure rate distributions and point processes with decreasing failure intensity rate and is positive for the increasing failure rate distributions and point processes with increasing failure intensity rate. The value of zero corresponds to the exponential life distribution or the Homogeneous Poisson Process.

================================================================================
Title:  An approximate introduction to how zk-SNARKs are possible 
Date: Date not available
Content:
Special thanks to Dankrad Feist, Karl Floersch and Hsiao-wei Wang for feedback and review. Perhaps the most powerful cryptographic technology to come out of the last decade is general-purpose succinct zero knowledge proofs, usually called zk-SNARKs ("zero knowledge succinct arguments of knowledge"). A zk-SNARK allows you to generate a proof that some computation has some particular output, in such a way that the proof can be verified extremely quickly even if the underlying computation takes a very long time to run. The "ZK" ("zero knowledge") part adds an additional feature: the proof can keep some of the inputs to the computation hidden. For example, you can make a proof for the statement "I know a secret number such that if you take the word ‘cow', add the number to the end, and SHA256 hash it 100 million times, the output starts with 0x57d00485aa". The verifier can verify the proof far more quickly than it would take for them to run 100 million hashes themselves, and the proof would also not reveal what the secret number is. In the context of blockchains, this has two very powerful applications: But zk-SNARKs are quite complex; indeed, as recently as in 2014-17 they were still frequently called "moon math". The good news is that since then, the protocols have become simpler and our understanding of them has become much better. This post will try to explain how ZK-SNARKs work, in a way that should be understandable to someone with a medium level of understanding of mathematics. Note that we will focus on scalability; privacy for these protocols is actually relatively easy once the scalability is there, so we will get back to that topic at the end. Let us take the example that we started with: we have a number (we can encode "cow" followed by the secret input as an integer), we take the SHA256 hash of that number, then we do that again another 99,999,999 times, we get the output, and we check what its starting digits are. This is a huge computation. A "succinct" proof is one where both the size of the proof and the time required to verify it grow much more slowly than the computation to be verified. If we want a "succinct" proof, we cannot require the verifier to do some work per round of hashing (because then the verification time would be proportional to the computation). Instead, the verifier must somehow check the whole computation without peeking into each individual piece of the computation. One natural technique is random sampling: how about we just have the verifier peek into the computation in 500 different places, check that those parts are correct, and if all 500 checks pass then assume that the rest of the computation must with high probability be fine, too? Such a procedure could even be turned into a non-interactive proof using the Fiat-Shamir heuristic: the prover computes a Merkle root of the computation, uses the Merkle root to pseudorandomly choose 500 indices, and provides the 500 corresponding Merkle branches of the data. The key idea is that the prover does not know which branches they will need to reveal until they have already "committed to" the data. If a malicious prover tries to fudge the data after learning which indices are going to be checked, that would change the Merkle root, which would result in a new set of random indices, which would require fudging the data again... trapping the malicious prover in an endless cycle. But unfortunately there is a fatal flaw in naively applying random sampling to spot-check a computation in this way: computation is inherently fragile. If a malicious prover flips one bit somewhere in the middle of a computation, they can make it give a completely different result, and a random sampling verifier would almost never find out.  If tasked with the problem of coming up with a zk-SNARK protocol, many people would make their way to this point and then get stuck and give up. How can a verifier possibly check every single piece of the computation, without looking at each piece of the computation individually? But it turns out that there is a clever solution. Polynomials are a special class of algebraic expressions of the form: i.e. they are a sum of any (finite!) number of terms of the form \(c x^k\). There are many things that are fascinating about polynomials. But here we are going to zoom in on a particular one: polynomials are a single mathematical object that can contain an unbounded amount of information (think of them as a list of integers and this is obvious). The fourth example above contained 816 digits of tau, and one can easily imagine a polynomial that contains far more. Furthermore, a single equation between polynomials can represent an unbounded number of equations between numbers. For example, consider the equation \(A(x) + B(x) = C(x)\). If this equation is true, then it's also true that: And so on for every possible coordinate. You can even construct polynomials to deliberately represent sets of numbers so you can check many equations all at once. For example, suppose that you wanted to check: You can use a procedure called Lagrange interpolation to construct polynomials \(A(x)\) that give (12, 10, 15, 15) as outputs at some specific set of coordinates (eg. (0, 1, 2, 3)), \(B(x)\) the outputs (1, 8, 8, 13) on those same coordinates, and so forth. In fact, here are the polynomials: Checking the equation \(A(x) + B(x) = C(x)\) with these polynomials checks all four above equations at the same time. You can even check relationships between a large number of adjacent evaluations of the same polynomial using a simple polynomial equation. This is slightly more advanced. Suppose that you want to check that, for a given polynomial \(F\), \(F(x+2) = F(x) + F(x+1)\) within the integer range \(\{0, 1 ... 98\}\) (so if you also check \(F(0) = F(1) = 1\), then \(F(100)\) would be the 100th Fibonacci number). As polynomials, \(F(x+2) - F(x+1) - F(x)\) would not be exactly zero, as it could give arbitrary answers outside the range \(x = \{0, 1 ... 98\}\). But we can do something clever. In general, there is a rule that if a polynomial \(P\) is zero across some set \(S=\{x_1, x_2 ... x_n\}\) then it can be expressed as \(P(x) = Z(x) * H(x)\), where \(Z(x) =\) \((x - x_1) * (x - x_2) * ... * (x - x_n)\) and \(H(x)\) is also a polynomial. In other words, any polynomial that equals zero across some set is a (polynomial) multiple of the simplest (lowest-degree) polynomial that equals zero across that same set. Why is this the case? It is a nice corollary of polynomial long division: the factor theorem. We know that, when dividing \(P(x)\) by \(Z(x)\), we will get a quotient \(Q(x)\) and a remainer \(R(x)\) which satisfy \(P(x) = Z(x) * Q(x) + R(x)\), where the degree of the remainder \(R(x)\) is strictly less than that of \(Z(x)\). Since we know that \(P\) is zero on all of \(S\), it means that \(R\) has to be zero on all of \(S\) as well. So we can simply compute \(R(x)\) via polynomial interpolation, since it's a polynomial of degree at most \(n-1\) and we know \(n\) values (the zeroes at \(S\)). Interpolating a polynomial with all zeroes gives the zero polynomial, thus \(R(x) = 0\) and \(H(x)= Q(x)\). Going back to our example, if we have a polynomial \(F\) that encodes Fibonacci numbers (so \(F(x+2) = F(x) + F(x+1)\) across \(x = \{0, 1 ... 98\}\)), then I can convince you that \(F\) actually satisfies this condition by proving that the polynomial \(P(x) =\) \(F(x+2) - F(x+1) - F(x)\) is zero over that range, by giving you the quotient: \(H(x) = \frac{F(x+2) - F(x+1) - F(x)}{Z(x)}\) Where \(Z(x) = (x - 0) * (x - 1) * ... * (x - 98)\). You can calculate \(Z(x)\) yourself (ideally you would have it precomputed), check the equation, and if the check passes then \(F(x)\) satisfies the condition! Now, step back and notice what we did here. We converted a 100-step-long computation (computing the 100th Fibonacci number) into a single equation with polynomials. Of course, proving the N'th Fibonacci number is not an especially useful task, especially since Fibonacci numbers have a closed form. But you can use exactly the same basic technique, just with some extra polynomials and some more complicated equations, to encode arbitrary computations with an arbitrarily large number of steps. Now, if only there was a way to verify equations with polynomials that's much faster than checking each coefficient... And once again, it turns out that there is an answer: polynomial commitments. A polynomial commitment is best viewed as a special way to "hash" a polynomial, where the hash has the additional property that you can check equations between polynomials by checking equations between their hashes. Different polynomial commitment schemes have different properties in terms of exactly what kinds of equations you can check. Here are some common examples of things you can do with various polynomial commitment schemes (we use \(com(P)\) to mean "the commitment to the polynomial \(P\)"): It's worth noting that these primitives can be constructed from each other. If you can add and multiply, then you can evaluate: to prove that \(P(w) = z\), you can construct \(Q(x) = \frac{P(x) - z}{x - w}\), and the verifier can check if \(Q(x) * (x - w) + z \stackrel{?}{=} P(x)\). This works because if such a polynomial \(Q(x)\) exists, then \(P(x) - z = Q(x) * (x - w)\), which means that \(P(x) - z\) equals zero at \(w\) (as \(x - w\) equals zero at \(w\)) and so \(P(x)\) equals \(z\) at \(w\). And if you can evaluate, you can do all kinds of checks. This is because there is a mathematical theorem that says, approximately, that if some equation involving some polynomials holds true at a randomly selected coordinate, then it almost certainly holds true for the polynomials as a whole. So if all we have is a mechanism to prove evaluations, we can check eg. our equation \(P(x + 2) - P(x + 1) - P(x) = Z(x) * H(x)\) using an interactive game: As I alluded to earlier, we can make this non-interactive using the Fiat-Shamir heuristic: the prover can compute r themselves by setting r = hash(com(P), com(H)) (where hash is any cryptographic hash function; it does not need any special properties). The prover cannot "cheat" by picking P and H that "fit" at that particular r but not elsewhere, because they do not know r at the time that they are picking P and H! There are three major schemes that are widely used at the moment: bulletproofs, Kate and FRI. To be honest, they're not that simple. There's a reason why all this math did not really take off until 2015 or so. In my opinion, the easiest one to understand fully is FRI (Kate is easier if you're willing to accept elliptic curve pairings as a "black box", but pairings are really complicated, so altogether I find FRI simpler). Here is how a simplified version of FRI works (the real protocol has many tricks and optimizations that are missing here for simplicity). Suppose that you have a polynomial \(P\) with degree \(< n\). The commitment to \(P\) is a Merkle root of a set of evaluations to \(P\) at some set of pre-selected coordinates (eg. \(\{0, 1 .... 8n-1\}\), though this is not the most efficient choice). Now, we need to add something extra to prove that this set of evaluations actually is a degree \(< n\) polynomial. Let \(Q\) be the polynomial only containing the even coefficients of \(P\), and \(R\) be the polynomial only containing the odd coefficients of \(P\). So if \(P(x) = x^4 + 4x^3 + 6x^2 + 4x + 1\), then \(Q(x) = x^2 + 6x + 1\) and \(R(x) = 4x + 4\) (note that the degrees of the coefficients get "collapsed down" to the range \([0...\frac{n}{2})\)). Notice that \(P(x) = Q(x^2) + x * R(x^2)\) (if this isn't immediately obvious to you, stop and think and look at the example above until it is). We ask the prover to provide Merkle roots for \(Q(x)\) and \(R(x)\). We then generate a random number \(r\) and ask the prover to provide a "random linear combination" \(S(x) = Q(x) + r * R(x)\). We pseudorandomly sample a large set of indices (using the already-provided Merkle roots as the seed for the randomness as before), and ask the prover to provide the Merkle branches for \(P\), \(Q\), \(R\) and \(S\) at these indices. At each of these provided coordinates, we check that: If we do enough checks, then we can be convinced that the "expected" values of \(S(x)\) are different from the "provided" values in at most, say, 1% of cases. Notice that \(Q\) and \(R\) both have degree \(< \frac{n}{2}\). Because \(S\) is a linear combination of \(Q\) and \(R\), \(S\) also has degree \(< \frac{n}{2}\). And this works in reverse: if we can prove \(S\) has degree \(< \frac{n}{2}\), then the fact that it's a randomly chosen combination prevents the prover from choosing malicious \(Q\) and \(R\) with hidden high-degree coefficients that "cancel out", so \(Q\) and \(R\) must both be degree \(< \frac{n}{2}\), and because \(P(x) = Q(x^2) + x * R(x^2)\), we know that \(P\) must have degree \(< n\). From here, we simply repeat the game with \(S\), progressively "reducing" the polynomial we care about to a lower and lower degree, until it's at a sufficiently low degree that we can check it directly. As in the previous examples, "Bob" here is an abstraction, useful for cryptographers to mentally reason about the protocol. In reality, Alice is generating the entire proof herself, and to prevent her from cheating we use Fiat-Shamir: we choose each randomly samples coordinate or r value based on the hash of the data generated in the proof up until that point. A full "FRI commitment" to \(P\) (in this simplified protocol) would consist of: Each step in the process can introduce a bit of "error", but if you add enough checks, then the total error will be low enough that you can prove that \(P(x)\) equals a degree \(< n\) polynomial in at least, say, 80% of positions. And this is sufficient for our use cases. If you want to cheat in a zk-SNARK, you would need to make a polynomial commitment for a fractional expression (eg. to "prove" the false claim that \(x^2 + 2x + 3\) evaluated at \(4\) equals \(5\), you would need to provide a polynomial commitment for \(\frac{x^2 + 2x + 3 - 5}{x - 4} = x + 6 + \frac{22}{x - 4}\)). The set of evaluations for such a fractional expression would differ from the evaluations for any real degree \(< n\) polynomial in so many positions that any attempt to make a FRI commitment to them would fail at some step. Also, you can check carefully that the total number and size of the objects in the FRI commitment is logarithmic in the degree, so for large polynomials, the commitment really is much smaller than the polynomial itself. To check equations between different polynomial commitments of this type (eg. check \(A(x) + B(x) = C(x)\) given FRI commitments to \(A\), \(B\) and \(C\)), simply randomly select many indices, ask the prover for Merkle branches at each of those indices for each polynomial, and verify that the equation actually holds true at each of those positions. The above description is a highly inefficient protocol; there is a whole host of algebraic tricks that can increase its efficiency by a factor of something like a hundred, and you need these tricks if you want a protocol that is actually viable for, say, use inside a blockchain transaction. In particular, for example, \(Q\) and \(R\) are not actually necessary, because if you choose your evaluation points very cleverly, you can reconstruct the evaluations of \(Q\) and \(R\) that you need directly from evaluations of \(P\). But the above description should be enough to convince you that a polynomial commitment is fundamentally possible. In the descriptions above, there was a hidden assumption: that each individual "evaluation" of a polynomial was small. But when we are dealing with polynomials that are big, this is clearly not true. If we take our example from above, \(628x^{271} + 318x^{270} + 530x^{269} + ... + 69x + 381\), that encodes 816 digits of tau, and evaluate it at \(x=1000\), you get.... an 816-digit number containing all of those digits of tau. And so there is one more thing that we need to add. In a real implementation, all of the arithmetic that we are doing here would not be done using "regular" arithmetic over real numbers. Instead, it would be done using modular arithmetic. We redefine all of our arithmetic operations as follows. We pick some prime "modulus" p. The % operator means "take the remainder of": \(15\ \%\ 7 = 1\), \(53\ \%\ 10 = 3\), etc (note that the answer is always non-negative, so for example \(-1\ \%\ 10 = 9\)). We redefine \(x + y \Rightarrow (x + y)\) % \(p\) \(x * y \Rightarrow (x * y)\) % \(p\) \(x^y \Rightarrow (x^y)\) % \(p\) \(x - y \Rightarrow (x - y)\) % \(p\) \(x / y \Rightarrow (x * y ^{p-2})\) % \(p\) The above rules are all self-consistent. For example, if \(p = 7\), then: More complex identities such as the distributive law also hold: \((2 + 4) \cdot 3\) and \(2 \cdot 3 + 4 \cdot 3\) both evaluate to \(4\). Even formulas like \((a^2 - b^2)\) = \((a - b) \cdot (a + b)\) are still true in this new kind of arithmetic. Division is the hardest part; we can't use regular division because we want the values to always remain integers, and regular division often gives non-integer results (as in the case of \(3/5\)). We get around this problem using Fermat's little theorem, which states that for any nonzero \(x < p\), it holds that \(x^{p-1}\) % \(p = 1\). This implies that \(x^{p-2}\) gives a number which, if multiplied by \(x\) one more time, gives \(1\), and so we can say that \(x^{p-2}\) (which is an integer) equals \(\frac{1}{x}\). A somewhat more complicated but faster way to evaluate this modular division operator is the extended Euclidean algorithm, implemented in python here.  With modular math we've created an entirely new system of arithmetic, and it's self-consistent in all the same ways traditional arithmetic is self-consistent. Hence, we can talk about all of the same kinds of structures over this field, including polynomials, that we talk about in "regular math". Cryptographers love working in modular math (or, more generally, "finite fields") because there is a bound on the size of a number that can arise as a result of any modular math calculation - no matter what you do, the values will not "escape" the set \(\{0, 1, 2 ... p-1\}\). Even evaluating a degree-1-million polynomial in a finite field will never give an answer outside that set. Let's say we want to prove that, for some polynomial \(P\), \(0 \le P(n) < 2^{64}\), without revealing the exact value of \(P(n)\). This is a common use case in blockchain transactions, where you want to prove that a transaction leaves a balance non-negative without revealing what that balance is. We can construct a proof for this with the following polynomial equations (assuming for simplicity \(n = 64\)): The latter two statements can be restated as "pure" polynomial equations as follows (in this context \(Z(x) = (x - 0) * (x - 1) * ... * (x - 63)\)): The idea is that successive evaluations of \(P(i)\) build up the number bit-by-bit: if \(P(4) = 13\), then the sequence of evaluations going up to that point would be: \(\{0, 1, 3, 6, 13\}\). In binary, 1 is 1, 3 is 11, 6 is 110, 13 is 1101; notice how \(P(x+1) = P(x) * 2 + R(x)\) keeps adding one bit to the end as long as \(R(x)\) is zero or one. Any number within the range \(0 \le x < 2^{64}\) can be built up over 64 steps in this way, any number outside that range cannot. But there is a problem: how do we know that the commitments to \(P(x)\) and \(R(x)\) don't "leak" information that allows us to uncover the exact value of \(P(64)\), which we are trying to keep hidden? There is some good news: these proofs are small proofs that can make statements about a large amount of data and computation. So in general, the proof will very often simply not be big enough to leak more than a little bit of information. But can we go from "only a little bit" to "zero"? Fortunately, we can. Here, one fairly general trick is to add some "fudge factors" into the polynomials. When we choose \(P\), add a small multiple of \(Z(x)\) into the polynomial (that is, set \(P'(x) = P(x) + Z(x) * E(x)\) for some random \(E(x)\)). This does not affect the correctness of the statement (in fact, \(P'\) evaluates to the same values as \(P\) on the coordinates that "the computation is happening in", so it's still a valid transcript), but it can add enough extra "noise" into the commitments to make any remaining information unrecoverable. Additionally, in the case of FRI, it's important to not sample random points that are within the domain that computation is happening in (in this case \(\{0...64\}\)).
================================================================================
Title: Eth.xyz | View the ENS profile & NFTs of any .eth name
Date: Date not available
Content:
The universal shortlink for your web3 username, now available for .eth names. TL;DR we secured this domain to help you share what you want to the world. This was our first shot, let us know what you think. Eth.xyz is a service that showcases the public ENS profile and NFT collection of any .eth name on a simple, secure, and easily shareable URL. Just add .xyz to the end of any .eth name in any browser and visit the URL. Every .eth name has been granted automatic access to the eth.xyz feature. Simply add “.xyz” to the end of any .eth name in any web browser and the ENS profile will display. For example: the ENS profile for maaria.eth is accessible in any browser at maaria.eth.xyz. The profile content is automatically generated from publicly available information provided through ENS and is ultimately controlled by the relevant ENS user. Yes! Like this: https://eth.xyz/thingelstad.xyz or https://eth.xyz/d.mirror.xyz Profile pages for all .xyz and other TLDs are available as subpages of eth.xyz. To view the profile page for any other TLD, just add your domain to the end of https://eth.xyz/. This subpage method supports subdomains as well. Yes! Please see the instructions here. Using your eth.xyz profile as your Bluesky username can help consolidate your online presence. Eth.xyz profiles are essentially free landing pages for decentralized identities, acting as a central hub of personally curated information. Connecting your digital identity across platforms can increase the visibility of your online presence to a broader audience. If your Bluesky validation isn't working, there are a couple of things to check: You can visit your .eth subdomain profile using the subpage method: https://eth.xyz/sub.domain.eth. At the moment, subdomains do not work in the shortlink (i.e. sub.domain.eth.xyz). We’d like to have a feature that can help people explore .eth profiles. For now, here are a few .eth profiles to test out. You can also visit https://ethleaderboard.xyz to check out some of the most
        popular .eth holders on Twitter. Eth.xyz was created for the ENS community with love by the XYZ Registry, the company behind .xyz domain names. The project is open for contribution or feedback on GitHub. Send us a message @xyz on Twitter! Connect to the ENS Manager to update your .eth name’s information, such as your content, addresses, and text records. For each .eth name, the NFTs displayed are pulled via API from OpenSea. .XYZ is the generic top-level domain name (“gTLD” or “TLD”) that originally helped to pioneer the ENS and DNS integration in 2018. Since 2014, .xyz has disrupted the status quo by offering domain name choice and accessibility to internet users, becoming the most popular new domain ending worldwide. The XYZ Registry supports the blockchain technology community and shares their passion for pushing the envelope of the internet. We are enthusiastic about our continued collaboration with ENS. .XYZ is honored to be the domain of choice for many cutting edge technology initiatives, from Mirror.xyz, Login.xyz, and Proof.xyz, to Universe.xyz and many more. We support web3 and the expansion of naming opportunities, and we are committed to continue exploring opportunities to harmonize with the adoption of web3. We believe that we belong together in the metaverse. As small business owners, the founders of .xyz know the importance of being able to secure a memorable domain at a reasonable price. That’s why .xyz’s mission has always been to help entrepreneurs and innovators get online. Since 2014, .xyz has built a passionate community of individuals, small businesses, and organizations. .XYZ partners with more than 230 registrars, is the #1 new gTLD in usage1, and leads the industry with a successful anti‑abuse program. We have also stood strong through anti‑competitive legal actions, showing that innovation and choice prevails. Today, you can find a .xyz domain in practically every country and territory, and we hope to foster further innovation by supporting the evolution of naming. WAGMI 1Namestat.org We would like to hear your feedback on eth.xyz. We have published eth.xyz on GitHub and invite you to open issues or submit pull requests. You can also email us at . Nothing on this page implies any endorsement or affiliation between XYZ or ENS. Privacy Policy
================================================================================
Title: OSF
Date: Date not available
Content:

        For full functionality of this site it is necessary to enable JavaScript.
        Here are
         instructions for enabling JavaScript in your web browser.
      
================================================================================
Title: Against on-chain governance. Refuting (and rebuking) Fred Ehrsam’s… | by Vlad Zamfir | Medium
Date: Date not available
Content:
Sign up Sign in Sign up Sign in Vlad Zamfir Follow -- 36 Listen Share I’ve been thinking about blockchain governance for a long time, and recently my understanding of the process has become (I think) a lot more clear. I was (and still am) working on a blog post titled “Blockchain Governance 101”, which I hope will share some basic language for reasoning about governance. Before I managed to finish (the research is ongoing, and it’s not my top priority), Fred Ehrsam published a blog post “Blockchain Governance: Programming Our Future”, framing the discussion as a primarily design problem. I really don’t think blockchain governance (or governance in general) can be understood as a design problem. I think it’s almost always a mistake to imagine that you can design and institute a governance process, especially for an existing blockchain community with existing processes, and especially without adequate knowledge of the existing governance processes. This blog post is not an introduction to blockchain governance (that is still in the pipeline) and won’t give an overview of key concepts, issues or questions. Nor will it provide any blockchain governance solutions or suggestions. It is instead refutation of core positions that Fred takes in his blog, along with a warning about the dangers of suggesting that people should try to institute their favorite formalized governance model without clear regard for the existing blockchain governance processes. I think that Fred’s intentions are good, and the views he expressed in his blog post are understandable. But I also I think the blog is missing important concepts and context to the point that I consider it harmful, warranting a long-form response (i.e. a response outside of Tweet storms). Fred and I agree that blockchain governance is extremely important. I think it’s the second (rather than first) most important factor that will determine whether blockchains end up being a public good, or a menace to the public. Second only after the shared purpose of the blockchain community (a purpose that legitimate governance presumably is supposed to bear out). There is no question that governance decisions matter, and big time. And the structure of a blockchain governance process can dramatically effect governance outcomes. However, this does not make blockchain governance a design problem. Governance is a process. It involves players who participate in that process to produce decisions that affect the governed resources. These decisions can have a lasting impacts on many stakeholders. The participants coordinate around this process, and they build knowledge about the governance processes, about each other, about their incentives and about their states of knowledge. This knowledge can be tacit or formal, it can be local knowledge or it can be common knowledge. Participants can develop strong norms (e.g. no contentious hard forks). The information and incentives of participants constrains their participation, and they need to be understood in the context of the underlying culture, as well as their individual contexts. The information and incentives of participants can change over time, but they don’t change instantly, nor do they change independently of each other’s incentives and states of knowledge. The process of changing the way that something is governed is not magic, and in our case it’s also very human. So even if it was possible to come up with an ideal solution to “the governance design problem”, it might not be possible for the participants to adopt it. There are pre-existing constraints on participants’ ability to coordinate to adopt any proposed governance solutions. And these constraints can even be entirely in their heads, in the form of local or tacit knowledge, or norms and common knowledge. So when you make a governance proposal, the participants in the current governance process are going to ask themselves “is this a significant improvement over the process we are using now?”, “Will the other participants in the process think so, too?”, “What will it take to switch from our current process to this one?”, and “Is it worth the disruption to existing processes?” Even if you somehow come up with an ideal governance design, you haven’t “solved governance” for anyone until it is successfully adopted. Fred seems to have missed the fact that information is just as important of a “critical component of governance” as incentives or “mechanisms for coordination” (which is a subset of a broader “critical component” called “governance structure”, from my point of view). Not a huge deal! When you propose an alternative governance process, and especially when you suggest that we need an alternative process, you’re proclaiming “the existing governance processes are not good enough” and you’re begging the question of “are the existing processes legitimate?”. You’re a stone’s throw away from advocating for revolution; a replacement of the existing governance processes. Revolution isn’t easy or risk-free, and forking means that you will leave some of your peers behind. When you are advocating for revolution, you had better be sure that the revolution will succeed, and that the new process will be effective and legitimate Fred’s blog post showed no almost acknowledgement of the existence of functoning blockchain governance processes in either Bitcoin or Ethereum. He dismissed them out-of-hand with little consideration, perhaps because Fred knows that the reader likely already assumes that Bitcoin governance is broken and that Vitalik controls Ethereum governance. He showed very little knowledge about how the Ethereum governence process works. That’s fair enough, because the Ethereum governance process are not very well documented, and it’s hard to understand them without actively participating in them. They evolved over time, and are not an institutionalization of a formal model, and therefore have no inherent reason to be easy to identify or communicate. No one has full information about the structure of the processes involved. Partly because documenting reality is hard work, and so is communication and education. Partly because the processes are still evolving. And also partly because people only inevitably learn about the processes that they participate in themselves. Most observers don’t participate, and can’t be expected to understand the process, at least until clear documentation is available. To clear up some of the misinformation about Ethereum governance shared in Fred’s blog: Proof-of-stake will not change the Ethereum governance processes at all, and miners *do not* have a significant influence on the governance process today. Moreover, Vitalik does not have nearly the amount of power to influence governance outcomes that Fred (and lots of other people) assume that he does. The structure of the governance processes limit Vitalik’s power, just as it limits everyone’s power. Unfortunately for the curious reader, documenting the Ethereum governance process is out of scope for this blog. Stay patient! :) “On-chain governance” refers to the idea that the blockchain nodes automatically upgrade when an on-chain governance process decides on an upgrade and that it’s time to install it. No hard forks required. Adopting on-chain governance is incredibly risky because it always represents a revolution. It’s not necessarily a revolt against the governance processes that merge code into software repositories (as those could conceivably be encoded on-chain, though this notably isn’t normally what is proposed), but a revolution that overthrows the processes that govern full nodes. With off-chain governance (the current norm), a node operator has to consciously decide whether to install a hard fork to have his node be consensus-compatible with the nodes of operators who also decided to install that hard fork. With off-chain blockchain governance, node operators’ decision processes are absolutely necessary parts of blockchain governance, and therefore node operators are necessary participants in blockchain governance. On-chain governance makes node operator participation in governance completely unnecessary. It makes it so that a node operator, making no decision, follows the decisions made by the on-chain process. Defaults are incredibly powerful: The more nodes follow the default, the less feasible it is for a concerned node operator to refuse to install a hard fork. (Technically, it’s not actually a hard fork or a soft fork, though the upgrade would have been a hard or soft fork in an off-chain governance model.) Why is this a big deal? Well, the protocol doesn’t have an anti-Sybil measure on node operators (or on their users). This means that node operators (and therefore users) will necessarily be robbed of their participation in governance, by any on-chain governance proposal. The role of full nodes in off-chain governance provides an important check to balance against the power of processes that make changes to software. On-chain governance removes that check, and the balance it provides. Unless there are governance processes that get Sybil-resistant input from node operators, on-chain governance therefore has always has the potential to disenfranchise node operators (and users) of the blockchain. If you are a blockchain node operator (or user), or if you care about blockchain node operators (or users), then I hope you will learn to regard on-chain governance proposals with extreme apprehension. Coin holder interests and user interest are not naturally aligned. Users have to buy coins from coin holders to use the blockchain. Coin holders would prefer if users had to pay more. While users would prefer if they had to pay less. It is critical to recognize that “user” and “coin holder” are roles that have distinct incentives corresponding to their roles. Just because you’re both a user and a coin holder, or even if most users today are also coin holders, does not mean that we should design our blockchain protocol or our blockchain governance processes to favour coin holder interests over user interests. The market between blockchains is absolutely not perfectly competitive. It is highly oligopolistic because blockchains have strong network effects (obviously because they are p2p protocols, and more subtly because different blockchain communities have different cultures). So it’s very risky to assume that coin holders want what’s good for users because the price of coins will go down as a result. That will only happen if the users bear sufficient cost. And that cost might be high, and even if it isn’t, any cost is something I wouldn’t wish on all of the users of the blockchain. If on-chain governance is a big risk to node operators and users, then plutocratic on-chain governance is at least as risky. And any on-chain governance proposal that is driven by coin holder votes has this problem. Yes, even if it’s based on prediction markets (futarchy), and even if the coins are locked up. Not only is on-chain coin-based governance inconsistent with user interests, it is also antithetical to the ethos of public blockchains. The blockchain is for the public, to serve the public interest. It isn’t for cryptocurrency whales to get more rich. Cyptocurrency holdings (like wealth in global society) is highly concentrated in the hands of a very small number of people. The blockchain isn’t supposed to be owned by anyone… nevermind by a small group of superrich individuals. Blockchain governance is too important for us to let a small handful of cryptocurrency whales make arbitrary decisions. I agree with Fred that the blockchain itself is a tremendously valuable tool for experimenting with governance tools and processes. I am a fan (in theory) of on-chain tools that are in smart contracts but aren’t part of the blockchain protocol. And I think we should experiment with that. I also agree that off-chain tools can be extremely valuable as a way for participants to signal to each other. These kinds of tools can provide a signal to node operators that actually helps them make a decision, as opposed to on-chain processes that make their decision unnecessary. These tools can also help participants in the governance processes that affect changes to software repositories. If we find that we can build useful blockchain governance tools using the blockchain, that’s great! However, overthrowing the processes that govern the software implementations of the blockchain, or the processes that govern full nodes is most probably not well-advised. Blockchain governance is not an abstract design problem. It’s an applied social problem. It’s a problem that is defined in the context of existing governance structure, and in the context of the current information and incentives of today’s participants in today’s governance processes. We need to look very carefully at the blockchain governance processes that we already have before we declare that they don’t exist, are illegitimate, propose alternatives, fork, or advocate for revolution. It is your right to propose the institutionalization of your favorite formal governance model as a replacement to the existing structure of governance, of course. However, consider that insodoing you may be sabotaging the legitimacy of the existing governance process. The existing process may have evolved over time and may not be well understood or documented. Replacing something you don’t understand with something you do has obvious appeal, but it is reckless. So maybe hold off until recklessness is the only tenable option? Please be careful. The effectiveness and legitimacy of our blockchain governance processes are critically important. Don’t needlessly put them at risk! Treat your articulation of governance problems and proposals as a loaded weapon and don’t shoot in the dark. -- -- 36 Absurdist, troll. Help Status About Careers Press Blog Privacy Terms Text to speech Teams
================================================================================
Title: Chainlink: The Industry-Standard Web3 Services Platform
Date: Date not available
Content:
The universal platform for pioneering the future of global markets onchain. Time-tested security Chainlink services are powered by decentralized oracle networks with a long track record of high availability, reliability, and data accuracy. Universal interoperability Chainlink connects existing systems to any public or private blockchain and enables secure cross-chain communication. World-class developer experience Chainlink provides developers across all major blockchains with extensive documentation, hands-on tutorials, and in-depth workshops.
================================================================================
Title: Ethereum Classic
Date: Aug 29, 2024
Content:
FAQs Frequently asked questions by various stakeholders Why Classic? Start here to get the lowdown on Ethereum Classic's reason for being and unique value proposition Knowledge Further reading on the foundations that underpin ETC Videos A collection of videos and podcasts to keep you informed on ETC concepts and happenings Services Apps, Wallets, and Exchanges that integrate with ETC; 100% unaudited, so watch out for the rug pulls! Community If you long for communication with people who are interested in ETC, here's how to find them Network RPC Endpoints, Explorers, Network Monitors, and other resources for the technically inclined Mining Pools, Software, and Hardware requirements will help you mine ETC for fun and profit Development Testnets, Tooling, Clients, and information on how to contribute to the ETC protocol itself External links are unofficial community submissions. External links are unofficial community submissions. As incumbent institutions are increasingly threatened by the wild potential of blockchain tech, only truly decentralized and secure cryptocurrencies will survive. Existing in response to contract censorship on sister chain Ethereum™ (ETH) and to uphold and preserve the principle of Code is Law, Ethereum Classic (ETC) has proven its ability to resist censorship against all odds, and deliver the original Ethereum vision of unstoppable applications. By combining the technology of Ethereum™ (ETH) with the philosophy of Bitcoin (BTC), Ethereum Classic (ETC) is uniquely positioned to be the base layer smart contracts platform of the future, as other chains become compromised or captured by special interests. Welcome, and please be invited to discover the past, present, and future of Ethereum Classic; its principles, value proposition, community, and more - to understand why Classic. Made with <3 for the Original Ethereum Vision
================================================================================
Title: Vitalik Buterin : Things that matter outside of defi - YouTube
Date: Date not available
Content:
Vitalik Buterin : Things that matter outside of defi - YouTubeAcerca dePrensaDerechos de autorComunicarte con nosotrosCreadoresAnunciarDesarrolladoresCondicionesPrivacidadPolíticas y seguridadCómo funciona YouTubePrueba funciones nuevas© 2024 Google LLC
================================================================================
Title: scalability_paper/scalability.pdf at master · vbuterin/scalability_paper · GitHub
Date: Date not available
Content:
We read every piece of feedback, and take your input very seriously. 
            To see all available qualifiers, see our documentation.
          
================================================================================
Title: x.com
Date: Date not available
Content:



x.com


































================================================================================
Title: Finance worker pays out $25 million after video call with deepfake ‘chief financial officer’ | CNN
Date: Date not available
Content:

            A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company’s chief financial officer in a video conference call, according to Hong Kong police.
     
            The elaborate scam saw the worker duped into attending a video call with what he thought were several other members of staff, but all of whom were in fact deepfake recreations, Hong Kong police said at a briefing on Friday.
     
            “(In the) multi-person video conference, it turns out that everyone [he saw] was fake,”  senior superintendent Baron Chan Shun-ching told the city’s public broadcaster RTHK.
     
            Chan said the worker had grown suspicious after he received a message that was purportedly from the company’s UK-based chief financial officer. Initially, the worker suspected it was a phishing email, as it talked of the need for a secret transaction to be carried out.
     
            However, the worker put aside his early doubts after the video call because other people in attendance had looked and sounded just like colleagues he recognized, Chan said.
     
            Believing everyone else on the call was real, the worker agreed to remit a total of $200 million Hong Kong dollars – about $25.6 million, the police officer added.
     
            The case is one of several recent episodes in which fraudsters are believed to have used deepfake technology to modify publicly available video and other footage to cheat people out of money.
     
            At the press briefing Friday, Hong Kong police said they had made six arrests in connection with such scams.
     
            Chan said that eight stolen Hong Kong identity cards – all of which had been reported as lost by their owners – were used to make 90 loan applications and 54 bank account registrations between July and September last year.
     
            On at least 20 occasions, AI deepfakes had been used to trick facial recognition programs by imitating the people pictured on the identity cards, according to police.
     
            The scam involving the fake CFO was only discovered when the employee later checked with the corporation’s head office.
     
Related article
The deepfake era of US politics is upon us
 
            Hong Kong police did not reveal the name or details of the company or the worker.
     
            Authorities across the world are growing increasingly concerned at the sophistication of deepfake technology and the nefarious uses it can be put to.
     
            At the end of January, pornographic, AI-generated images of the American pop star Taylor Swift spread across social media, underscoring the damaging potential posed by artificial intelligence technology.
     
            The photos - which show the singer in sexually suggestive and explicit positions - were viewed tens of millions of times before being removed from social platforms.
     © 2024 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network.
================================================================================
